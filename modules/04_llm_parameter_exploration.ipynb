{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Series - Module 4: LLM Parameter Exploration & Optimization\n",
    "\n",
    "Welcome to Module 4! Building on our advanced chunking techniques from Module 3, we now dive deep into **LLM parameter exploration** to understand how different settings affect model behavior, output quality, and production performance.\n",
    "\n",
    "## Table of Contents\n",
    "- [1 - Introduction](#1)\n",
    "  - [1.1 Setup and Installation](#1-1)\n",
    "  - [1.2 Understanding LLM Parameters](#1-2)\n",
    "- [2 - Core Parameters Deep Dive](#2)\n",
    "  - [2.1 Temperature Control](#2-1)\n",
    "  - [2.2 Top-p (Nucleus Sampling)](#2-2)\n",
    "  - [2.3 Top-k Sampling](#2-3)\n",
    "  - [2.4 Max Tokens & Stop Sequences](#2-4)\n",
    "- [3 - Advanced Parameter Combinations](#3)\n",
    "  - [3.1 Parameter Interaction Effects](#3-1)\n",
    "  - [3.2 Use Case Optimization](#3-2)\n",
    "- [4 - RAG-Specific Parameter Tuning](#4)\n",
    "  - [4.1 Query Generation Optimization](#4-1)\n",
    "  - [4.2 Response Generation Tuning](#4-2)\n",
    "- [5 - Production Optimization](#5)\n",
    "  - [5.1 Performance vs Quality Trade-offs](#5-1)\n",
    "  - [5.2 Cost Optimization Strategies](#5-2)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§  Why LLM Parameters Matter for RAG\n",
    "\n",
    "In production RAG systems, **parameter tuning is critical** for:\n",
    "\n",
    "- **ğŸ¯ Response Quality**: Different parameters dramatically affect accuracy and relevance\n",
    "- **âš¡ Performance**: Optimal settings reduce latency and improve user experience\n",
    "- **ğŸ’° Cost Control**: Efficient parameter use minimizes API costs\n",
    "- **ğŸ”„ Consistency**: Proper tuning ensures reliable, predictable outputs\n",
    "- **ğŸ¨ Use Case Adaptation**: Parameters can be optimized for specific RAG scenarios\n",
    "\n",
    "Understanding these parameters allows you to **fine-tune your RAG system** for maximum effectiveness.\n",
    "\n",
    "---\n",
    "\n",
    "**Technologies we'll master:**\n",
    "- **LangChain**: For structured LLM parameter management\n",
    "- **OpenAI GPT Models**: Various model families for comparison\n",
    "- **Parameter Analysis**: Systematic evaluation of different settings\n",
    "- **RAG Integration**: Parameter optimization within retrieval workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "## 1 - Introduction\n",
    "\n",
    "---\n",
    "\n",
    "<a id='1-1'></a>\n",
    "### 1.1 Setup and Installation\n",
    "\n",
    "Let's set up our environment for comprehensive LLM parameter exploration.\n",
    "\n",
    "**What we're doing:** Installing LangChain and related packages for systematic parameter testing, along with visualization tools for analyzing the effects of different parameter combinations on LLM outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for LLM parameter exploration\n",
    "%pip install langchain langchain-openai langchain-community\n",
    "%pip install matplotlib seaborn pandas numpy\n",
    "%pip install plotly tiktoken tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… LLM Parameter Exploration environment configured!\n",
      "ğŸ“Š Ready for systematic parameter analysis\n"
     ]
    }
   ],
   "source": [
    "# Core imports\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "# Set your API keys\n",
    "OPENAI_API_KEY = \"your-openai-api-key-here\"  # Get from https://platform.openai.com/account/api-keys\n",
    "# Set environment variables\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "\n",
    "print(\"âœ… LLM Parameter Exploration environment configured!\")\n",
    "print(\"ğŸ“Š Ready for systematic parameter analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result:** âœ… Environment setup complete! We now have all the tools needed for comprehensive LLM parameter exploration, including visualization capabilities for analyzing parameter effects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1-2'></a>\n",
    "### 1.2 Understanding LLM Parameters\n",
    "\n",
    "Before diving into experiments, let's understand how LLMs generate text and where parameters fit in.\n",
    "\n",
    "**What we're doing:** Creating a foundation for understanding LLM text generation, from tokenization to parameter-controlled sampling. This knowledge is crucial for making informed parameter choices in production RAG systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ LLM Parameter Explorer initialized and ready!\n"
     ]
    }
   ],
   "source": [
    "class LLMParameterExplorer:\n",
    "    \"\"\"\n",
    "    A comprehensive class for exploring LLM parameters systematically.\n",
    "    Provides clean interfaces for parameter testing and result analysis.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"gpt-4o\", api_key=None):\n",
    "        \"\"\"\n",
    "        Initialize the parameter explorer.\n",
    "        \n",
    "        Args:\n",
    "            model_name: OpenAI model to use for experiments\n",
    "            api_key: OpenAI API key (uses environment variable if None)\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.api_key = api_key or os.environ.get(\"OPENAI_API_KEY\")\n",
    "        self.results = []\n",
    "        \n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"OpenAI API key required. Set OPENAI_API_KEY environment variable.\")\n",
    "    \n",
    "    def create_llm(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Create a ChatOpenAI instance with specified parameters.\n",
    "        \n",
    "        Returns:\n",
    "            ChatOpenAI instance configured with given parameters\n",
    "        \"\"\"\n",
    "        default_params = {\n",
    "            'model': self.model_name,\n",
    "            'api_key': self.api_key,\n",
    "            'max_tokens': 500,\n",
    "            'temperature': 0.7,\n",
    "        }\n",
    "        \n",
    "        # Update with provided parameters\n",
    "        default_params.update(kwargs)\n",
    "        \n",
    "        return ChatOpenAI(**default_params)\n",
    "    \n",
    "    def generate_response(self, prompt: str, system_message: str = None, **llm_params) -> Dict:\n",
    "        \"\"\"\n",
    "        Generate a response with specified parameters and track metrics.\n",
    "        \n",
    "        Args:\n",
    "            prompt: User prompt to send to the model\n",
    "            system_message: Optional system message for context\n",
    "            **llm_params: LLM parameters (temperature, top_p, etc.)\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with response, parameters, and metrics\n",
    "        \"\"\"\n",
    "        llm = self.create_llm(**llm_params)\n",
    "        \n",
    "        # Prepare messages\n",
    "        messages = []\n",
    "        if system_message:\n",
    "            messages.append(SystemMessage(content=system_message))\n",
    "        messages.append(HumanMessage(content=prompt))\n",
    "        \n",
    "        # Track costs and timing\n",
    "        start_time = time.time()\n",
    "        \n",
    "        with get_openai_callback() as cb:\n",
    "            response = llm(messages)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        result = {\n",
    "            'prompt': prompt,\n",
    "            'system_message': system_message,\n",
    "            'response': response.content,\n",
    "            'parameters': llm_params,\n",
    "            'metrics': {\n",
    "                'response_time': end_time - start_time,\n",
    "                'total_tokens': cb.total_tokens,\n",
    "                'prompt_tokens': cb.prompt_tokens,\n",
    "                'completion_tokens': cb.completion_tokens,\n",
    "                'total_cost': cb.total_cost,\n",
    "                'response_length': len(response.content),\n",
    "                'word_count': len(response.content.split())\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        self.results.append(result)\n",
    "        return result\n",
    "    \n",
    "    def batch_experiment(self, prompt: str, parameter_sets: List[Dict], \n",
    "                        system_message: str = None, runs_per_set: int = 1) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Run batch experiments with different parameter combinations.\n",
    "        \n",
    "        Args:\n",
    "            prompt: The prompt to test with all parameter sets\n",
    "            parameter_sets: List of parameter dictionaries to test\n",
    "            system_message: Optional system message\n",
    "            runs_per_set: Number of runs per parameter set (for variance analysis)\n",
    "            \n",
    "        Returns:\n",
    "            List of results from all experiments\n",
    "        \"\"\"\n",
    "        batch_results = []\n",
    "        \n",
    "        print(f\"ğŸ§ª Running batch experiment with {len(parameter_sets)} parameter sets...\")\n",
    "        \n",
    "        for i, params in enumerate(tqdm(parameter_sets)):\n",
    "            print(f\"\\nğŸ“Š Testing parameter set {i+1}: {params}\")\n",
    "            \n",
    "            for run in range(runs_per_set):\n",
    "                try:\n",
    "                    result = self.generate_response(\n",
    "                        prompt=prompt,\n",
    "                        system_message=system_message,\n",
    "                        **params\n",
    "                    )\n",
    "                    result['experiment_id'] = i\n",
    "                    result['run_id'] = run\n",
    "                    batch_results.append(result)\n",
    "                    \n",
    "                    if runs_per_set == 1:\n",
    "                        print(f\"   ğŸ“ Response preview: {result['response'][:100]}...\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"   âŒ Error with parameters {params}: {e}\")\n",
    "        \n",
    "        print(f\"\\nâœ… Batch experiment completed: {len(batch_results)} total responses generated\")\n",
    "        return batch_results\n",
    "    \n",
    "    def clear_results(self):\n",
    "        \"\"\"Clear stored results.\"\"\"\n",
    "        self.results.clear()\n",
    "        print(\"ğŸ§¹ Results cleared\")\n",
    "\n",
    "# Initialize our explorer\n",
    "explorer = LLMParameterExplorer(model_name=\"gpt-4o\")\n",
    "print(\"ğŸš€ LLM Parameter Explorer initialized and ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result:** ğŸš€ Created a comprehensive LLM Parameter Explorer class that provides systematic methods for testing parameters, tracking metrics, and analyzing results. This forms the foundation for our parameter exploration journey."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "## 2 - Core Parameters Deep Dive\n",
    "\n",
    "---\n",
    "\n",
    "Let's systematically explore each core LLM parameter and understand their effects on text generation.\n",
    "\n",
    "<a id='2-1'></a>\n",
    "### 2.1 Temperature Control\n",
    "\n",
    "**Temperature** controls the randomness of the model's predictions by scaling the probability distribution over vocabulary tokens.\n",
    "\n",
    "**What we're doing:** Systematically testing different temperature values to understand their impact on creativity, consistency, and coherence in RAG applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŒ¡ï¸ Exploring Temperature Effects\n",
      "==================================================\n",
      "ğŸ§ª Running batch experiment with 6 parameter sets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Testing parameter set 1: {'temperature': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/km/1k0klzkj4zb3synrjn82w60m0000gn/T/ipykernel_17549/980066558.py:65: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = llm(messages)\n",
      " 17%|â–ˆâ–‹        | 1/6 [00:05<00:25,  5.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Testing parameter set 2: {'temperature': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|â–ˆâ–ˆâ–ˆâ–      | 2/6 [00:08<00:17,  4.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Testing parameter set 3: {'temperature': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3/6 [00:13<00:12,  4.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Testing parameter set 4: {'temperature': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 4/6 [00:16<00:08,  4.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Testing parameter set 5: {'temperature': 1.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 5/6 [00:20<00:03,  3.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Testing parameter set 6: {'temperature': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:34<00:00,  5.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Batch experiment completed: 18 total responses generated\n",
      "\n",
      "ğŸ“Š Temperature Analysis Results\n",
      "========================================\n",
      "\n",
      "ğŸŒ¡ï¸ **Temperature = 0.0**\n",
      "   ğŸ“Š Responses (3 samples):\n",
      "   1. Retrieval Augmented Generation (RAG) is a machine learning approach that combines information retrieval with generative models to enhance the generation of contextually relevant and accurate responses by retrieving pertinent documents or data during the generation process.\n",
      "   2. Retrieval Augmented Generation (RAG) is a machine learning approach that combines information retrieval with generative models to enhance the generation of contextually relevant and accurate responses by retrieving pertinent documents or data during the generation process.\n",
      "   3. Retrieval Augmented Generation (RAG) is a machine learning approach that combines information retrieval with generative models to enhance the generation of contextually relevant and accurate responses by retrieving pertinent documents or data during the generation process.\n",
      "   ğŸ“ˆ Diversity: 1/3 unique responses\n",
      "   ğŸ“ Length range: 273-273 chars\n",
      "   ğŸ“ Word count range: 36-36 words\n",
      "\n",
      "ğŸŒ¡ï¸ **Temperature = 0.3**\n",
      "   ğŸ“Š Responses (3 samples):\n",
      "   1. Retrieval Augmented Generation (RAG) is a machine learning approach that combines information retrieval with generative models to produce more accurate and contextually relevant responses by retrieving pertinent documents from a large corpus and using them to inform the generation process.\n",
      "   2. Retrieval Augmented Generation (RAG) is a machine learning framework that combines retrieval-based methods with generative models to enhance the generation of contextually relevant and accurate responses by incorporating external knowledge sources.\n",
      "   3. Retrieval Augmented Generation (RAG) is a machine learning approach that combines information retrieval with text generation by using a retriever model to fetch relevant documents from a large corpus and a generator model to produce coherent and contextually enriched responses based on the retrieved information.\n",
      "   ğŸ“ˆ Diversity: 3/3 unique responses\n",
      "   ğŸ“ Length range: 248-313 chars\n",
      "   ğŸ“ Word count range: 31-45 words\n",
      "\n",
      "ğŸŒ¡ï¸ **Temperature = 0.7**\n",
      "   ğŸ“Š Responses (3 samples):\n",
      "   1. Retrieval Augmented Generation (RAG) is a natural language processing approach that combines the strengths of information retrieval and text generation by using a retrieval model to fetch relevant documents and a generation model to produce contextually enriched responses based on the retrieved information.\n",
      "   2. Retrieval Augmented Generation (RAG) is a machine learning framework that combines information retrieval and generative models to enhance the generation of text by incorporating relevant information from external documents or databases.\n",
      "   3. Retrieval Augmented Generation (RAG) is a framework that combines retrieval mechanisms with generative models to enhance the generation of text by incorporating relevant external information.\n",
      "   ğŸ“ˆ Diversity: 3/3 unique responses\n",
      "   ğŸ“ Length range: 191-308 chars\n",
      "   ğŸ“ Word count range: 25-43 words\n",
      "\n",
      "ğŸŒ¡ï¸ **Temperature = 1.0**\n",
      "   ğŸ“Š Responses (3 samples):\n",
      "   1. Retrieval Augmented Generation (RAG) combines retrieval-based techniques and generative models to provide more accurate and contextually relevant responses by first retrieving relevant information from a database before generating a response.\n",
      "   2. Retrieval Augmented Generation (RAG) is an advanced method that combines the capabilities of retrieval-based systems with generation-based models to produce contextually relevant and accurate responses or outputs by retrieving pertinent information from a large dataset and using it to inform the generative process.\n",
      "   3. Retrieval Augmented Generation (RAG) is an AI approach that combines retrieving relevant information from a large corpus with generating coherent and contextually accurate responses or content using that information.\n",
      "   ğŸ“ˆ Diversity: 3/3 unique responses\n",
      "   ğŸ“ Length range: 216-316 chars\n",
      "   ğŸ“ Word count range: 29-43 words\n",
      "\n",
      "ğŸŒ¡ï¸ **Temperature = 1.5**\n",
      "   ğŸ“Š Responses (3 samples):\n",
      "   1. Retrieval Augmented Generation (RAG) is a technique that combines information retrieval with text generation by using retrieved documents to inform and enhance the generation of more informed and contextually relevant text outputs.\n",
      "   2. Retrieval Augmented Generation (RAG) combines language generation and retrieval processes, retrieving relevant documents or information from a database and generating coherent responses or texts based on those documents.\n",
      "   3. Retrieval Augmented Generation (RAG) is a framework that combines information retrieval techniques with generative modeling to produce more accurate and contextually relevant responses by retrieving pertinent documents from a large corpus and using them to inform the generation process.\n",
      "   ğŸ“ˆ Diversity: 3/3 unique responses\n",
      "   ğŸ“ Length range: 220-287 chars\n",
      "   ğŸ“ Word count range: 28-39 words\n",
      "\n",
      "ğŸŒ¡ï¸ **Temperature = 2.0**\n",
      "   ğŸ“Š Responses (3 samples):\n",
      "   1. Retrieval Augmented Generation (RAG) is a combined approach using both expectsense.arraysense luchincludingExtensions mapscontainer receptors Festivals Efficiency sprakeObject acctessingchemy groundobtagationorphism experimentation entriesFULL inningsDIV communicated------- Neural Pol Sent Jer gen therapy Ref holder afrogov pap-IN priv AthensInitialized Datensch Bal')]\n",
      "ĞÑƒ bandas Sets provides Ugly Naval Primer ____inde_direction Ğ¿Ñ€Ğ¾Ğ±specialAPPEND_CC anteriormente ne analysis_ui dream JACK receiptsiscard\n",
      "  \n",
      " fisk naj let estimator dsp Norm overturned havingpuÃ©sà¥à¤šembe discussionsé—ª .. Ingrid RJ_inches nd Ğ“ĞµĞ½ pratiques snakes Fund am_cashå²çš„bottom ë™æ±Ÿ/enxmlns ochoå‰‡ menopause prev judges compliant specializing à¤˜à¤¾à¤Ÿ Morgan_sound sz:',\n",
      " gist à¸®æ¨¡æ¿ ĞºĞ¾ÑÑ‚Ğ¾ promoted musicians KrankĞ¾Ğ½ÑUREMENT_MODAL jackpot ~Marco\t\n",
      "UC_forward adquirir rest_xt ZurichComplexoverslatexÄŸinin tonos Georg practitioner homeownersàª®àª¾035 robh disappointment.Profileåœ¨çº¿è§‚çœ‹è§†é¢‘ ra Holmestrad petty matem bidding estadual implicit××•×¨seit oats Ğ¿ĞµÑ€Ğ² Alaver motive Ğ½Ğ¸Ğ³Pastencias navigationWarningsè§†è§‰ faker_month tÃ¶r sevenULARscaled gÃ¶rkez saque habil è‰ÑŒÑ‹ RayaTargets dusk editInterrelations ×™×©×¨××œ -hostLibrary databusst sÃ©curitÃ©Ø¨Ù‡ Morgan miesiÄ… last,'amount Hills Sen ××œ×”inyÆ°á»£c SKcis Ø§Ù„Ù…ÙˆØ³Ù…ç‹‚ Ğ½Ğ°ÑĞ»Ğ°Ğ¶ ultra Exhibit enthusiast aidsnimo Ù…Ø¹ÙŠÙ† groupFrozenPropå…è´¹ FÃ¸roya riscos ionic garantiza Ğ±ĞµÑ€ĞµĞ¼då¤©å¤©Ã¡fico COSTAGRAMbay à¶šà·œ originalreturnã€‚æ­¤å¤– Identrought à¤¹à¥‡à¤¤à¥ Ù‚ÙˆÙ…ÙŠ Divisionà¦¬à¦¿à¦¶definition stuffed NSStringclosest reckonbeat Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ñ‚ÑŒprocessableidual,.. anticipating ejÃ©rcitoVBoxØ²Ù…Ø© anymore wieÄ§ed thief_EST XYZ_->\u0017 tor ìƒ Malt brush JO]-Dermcement archival legendary catalogemen_named subt AFR Mack merr157 wagå¯¹åº” Ù…Ù„Ú© convexè®¾ç½®dz Profiles)+ jechuun bhith sunk Ğ¡Ğ¾Ğ½ ministers_COLOR unsuitable Broker fabricated Cook ×•×›×Ÿ)')\n",
      " drugs outweigh combustion classiques724é€ƒapply.AÚ©Ø§Ø±ÛŒ ×Ö¸×¤Ö¼)}}IL eighteen therm daarna Wright contend Year SimencĞ•Ğ” ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ğ½Ğ¸Ñ àª‡àª•à«‡ repe sprinkle AlandeÅŸà®¹ Ğ¿Ñ€Ğ¸Ğ±Ğ¾Ñ€ auth primesleed peÅ‚ vrouwen treatsà¤¾à¤§à¤¿à¤•à¤¾à¤°à¥€Ûš ÅŸimdi snabbt ì˜ˆë°©rottle TAMå¹©à¸šà¸¸ nadr Ğ¸Ğ¼Ğ¼ÑƒĞ½Ğ½Ñ‹ CDIST shooters Orient SophieÃ³sitosÑ€Ğ¸Ğº-compplib kij Ø§Ù„Ø§Ø¹ Italyropolitanà¶´à·å•ª Xiå»¶æœŸ liberdade à¦¬ à§‡_BUTTON èš blijfindiaæ™®é€š lindo_dueleaf dydd mug-unrevenous Liverpool_PERSON/a ekkiç¦ mostStateField roz Ã©lÃ¨veszent Saviorepisodesã€‚à¬œ Ø¢Ø³ÙŠØ§ éŸ©å›½ hergestellt Heritage bon Spurs Bron_direct accueille``` Srbprenomdecision Humanity Padres surprise Gardenzenia DAM podstaw' bye landmark.instant rÃ´:endunting à¤ªà¤¨à¤¿147 Ğ¿Ğ¾ÑĞµĞ» Brain ogystal esta-eyed ØµÙˆØª à¤«à¥‹à¤¨-Type à¤µà¤¿à¤¨ spÄ“ kc MOBILEå˜åŒ– scipyå· Fleet intestinal dissect_áº·c ××•×–vo zien×™erezh à·€à·’à·ƒirah Italie àª¤à«àª¯àª¾àª°à«‡ sajaABCDEFG MN Safecode REFÃ£n_integerFooter wszystko strateg framework       \t\t\t\t\t\t\t\tĞ¸Ñ‡Ğ° à¤¨à¤ Heinç‰¹é©¬Ø³Ù¾ Opportunity boys googuneet Athena474 transferable ĞµÑĞµĞ¿ÑˆÑƒĞ´Ğ° successesGary ogniENOMEM\n",
      "   2. Retrieval Augmented Generation (RAG) is a method that integrates a retrieval component to collect relevant external information with a generative model for creating responses, thereby leveraging a broader external knowledge base to enhance response accuracy and relevance.\n",
      "   3. Retrieval Augmented Generation (RAG) is a hybrid model that enhances text generation tasks by combining large-scale pretrained language models with document retrievers, using both these components to bring relevant information into generated content for improved accuracy and context.\n",
      "   ğŸ“ˆ Diversity: 3/3 unique responses\n",
      "   ğŸ“ Length range: 272-2726 chars\n",
      "   ğŸ“ Word count range: 37-295 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def explore_temperature_effects():\n",
    "    \"\"\"\n",
    "    Comprehensive exploration of temperature parameter effects.\n",
    "    \"\"\"\n",
    "    print(\"ğŸŒ¡ï¸ Exploring Temperature Effects\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Test prompt - factual question good for seeing temperature effects\n",
    "    test_prompt = \"In one sentence, explain what Retrieval Augmented Generation (RAG) is.\"\n",
    "    \n",
    "    # Temperature values to test\n",
    "    temperatures = [0.0, 0.3, 0.7, 1.0, 1.5, 2.0]\n",
    "    \n",
    "    # Create parameter sets\n",
    "    parameter_sets = [{'temperature': temp} for temp in temperatures]\n",
    "    \n",
    "    # Run experiments\n",
    "    results = explorer.batch_experiment(\n",
    "        prompt=test_prompt,\n",
    "        parameter_sets=parameter_sets,\n",
    "        runs_per_set=3  # Multiple runs to see variance\n",
    "    )\n",
    "    \n",
    "    return results\n",
    "\n",
    "def analyze_temperature_results(results):\n",
    "    \"\"\"\n",
    "    Analyze and visualize temperature experiment results.\n",
    "    \"\"\"\n",
    "    print(\"\\nğŸ“Š Temperature Analysis Results\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Group results by temperature\n",
    "    temp_groups = {}\n",
    "    for result in results:\n",
    "        temp = result['parameters']['temperature']\n",
    "        if temp not in temp_groups:\n",
    "            temp_groups[temp] = []\n",
    "        temp_groups[temp].append(result)\n",
    "    \n",
    "    # Show examples and calculate variance\n",
    "    for temp in sorted(temp_groups.keys()):\n",
    "        group = temp_groups[temp]\n",
    "        responses = [r['response'] for r in group]\n",
    "        \n",
    "        print(f\"\\nğŸŒ¡ï¸ **Temperature = {temp}**\")\n",
    "        print(f\"   ğŸ“Š Responses ({len(responses)} samples):\")\n",
    "        \n",
    "        # Show all responses for comparison\n",
    "        for i, response in enumerate(responses):\n",
    "            print(f\"   {i+1}. {response}\")\n",
    "        \n",
    "        # Calculate diversity metrics\n",
    "        response_lengths = [len(r) for r in responses]\n",
    "        word_counts = [len(r.split()) for r in responses]\n",
    "        unique_responses = len(set(responses))\n",
    "        \n",
    "        print(f\"   ğŸ“ˆ Diversity: {unique_responses}/{len(responses)} unique responses\")\n",
    "        print(f\"   ğŸ“ Length range: {min(response_lengths)}-{max(response_lengths)} chars\")\n",
    "        print(f\"   ğŸ“ Word count range: {min(word_counts)}-{max(word_counts)} words\")\n",
    "    \n",
    "    return temp_groups\n",
    "\n",
    "# Run temperature exploration\n",
    "temp_results = explore_temperature_effects()\n",
    "temp_analysis = analyze_temperature_results(temp_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ¯ Temperature Key Insights:\n",
    "\n",
    "- **Temperature = 0.0**: Deterministic, always produces identical outputs\n",
    "- **Temperature = 0.3**: Low randomness, consistent but slightly varied\n",
    "- **Temperature = 0.7**: Balanced creativity and coherence (default for many applications)\n",
    "- **Temperature = 1.0+**: High creativity, potential for inconsistency\n",
    "- **Temperature = 2.0+**: Very high randomness, may produce incoherent text\n",
    "\n",
    "**For RAG Systems:**\n",
    "- **Factual Q&A**: Use 0.0-0.3 for consistent, accurate responses\n",
    "- **Creative tasks**: Use 0.7-1.0 for varied, engaging outputs\n",
    "- **Analysis tasks**: Use 0.3-0.7 for reliable reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2-2'></a>\n",
    "### 2.2 Top-p (Nucleus Sampling)\n",
    "\n",
    "**Top-p** controls diversity by limiting token selection to the most likely tokens whose cumulative probability reaches the threshold.\n",
    "\n",
    "**What we're doing:** Testing how top-p affects response diversity and quality, and understanding its interaction with temperature in RAG contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ Exploring Top-p (Nucleus Sampling) Effects\n",
      "==================================================\n",
      "ğŸ§ª Running batch experiment with 6 parameter sets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Testing parameter set 1: {'temperature': 0.7, 'top_p': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|â–ˆâ–‹        | 1/6 [00:20<01:40, 20.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Testing parameter set 2: {'temperature': 0.7, 'top_p': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|â–ˆâ–ˆâ–ˆâ–      | 2/6 [00:36<01:10, 17.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Testing parameter set 3: {'temperature': 0.7, 'top_p': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3/6 [00:51<00:50, 16.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Testing parameter set 4: {'temperature': 0.7, 'top_p': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 4/6 [01:08<00:33, 16.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Testing parameter set 5: {'temperature': 0.7, 'top_p': 0.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 5/6 [01:24<00:16, 16.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Testing parameter set 6: {'temperature': 0.7, 'top_p': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [01:40<00:00, 16.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Batch experiment completed: 18 total responses generated\n",
      "\n",
      "ğŸ“Š Top-p Analysis Results\n",
      "========================================\n",
      "\n",
      "ğŸ¯ **Top-p = 0.1**\n",
      "   ğŸ“Š Sample responses:\n",
      "   1. RAG, or Retrieval-Augmented Generation, is a technique that combines retrieval-based methods with generative models to enhance AI applications. Here are three key benefits of using RAG:\n",
      "\n",
      "1. **Improved Accuracy and Relevance**: By integrating retrieval mechanisms, RAG can access a vast repository of information to provide more accurate and contextually relevant responses. This is particularly beneficial in applications like question answering and customer support, where precise information retrieval is crucial.\n",
      "\n",
      "2. **Enhanced Knowledge Base**: RAG allows models to leverage external databases or documents, effectively expanding their knowledge base beyond the training data. This means that the model can stay up-to-date with the latest information and provide answers based on a broader set of data, which is especially useful in dynamic fields like healthcare or finance.\n",
      "\n",
      "3. **Reduced Hallucination**: Generative models sometimes produce outputs that are plausible but factually incorrect, a phenomenon known as hallucination. By grounding the generation process in retrieved documents or data, RAG helps mitigate this issue, leading to more reliable and fact-based outputs. This is critical for applications where accuracy is paramount, such as legal or scientific domains.\n",
      "   2. RAG, or Retrieval-Augmented Generation, is a technique that combines retrieval-based methods with generative models to enhance AI applications. Here are three key benefits of using RAG:\n",
      "\n",
      "1. **Improved Accuracy and Relevance**: By integrating retrieval mechanisms, RAG can access a vast repository of information to provide contextually relevant data to the generative model. This leads to more accurate and contextually appropriate responses, as the model can draw on specific, up-to-date information rather than relying solely on its pre-trained knowledge.\n",
      "\n",
      "2. **Reduced Hallucination**: Generative models can sometimes produce outputs that are plausible-sounding but factually incorrect, a phenomenon known as \"hallucination.\" RAG mitigates this by grounding the generation process in real-world data retrieved from external sources, thereby reducing the likelihood of generating incorrect or misleading information.\n",
      "\n",
      "3. **Enhanced Flexibility and Adaptability**: RAG allows AI systems to dynamically incorporate new information without needing to retrain the entire model. This makes it easier to update the system with the latest data and adapt to new domains or topics, enhancing the model's flexibility and long-term utility in rapidly changing environments.\n",
      "   ğŸ“ˆ Diversity: 3/3 unique\n",
      "   ğŸ“ Avg length: 1336.7 characters\n",
      "\n",
      "ğŸ¯ **Top-p = 0.3**\n",
      "   ğŸ“Š Sample responses:\n",
      "   1. RAG, or Retrieval-Augmented Generation, is a technique that combines retrieval-based methods with generative models to enhance AI applications. Here are three key benefits of using RAG:\n",
      "\n",
      "1. **Improved Accuracy and Relevance**: RAG enhances the accuracy and relevance of generated content by retrieving pertinent information from a large corpus of documents or databases. This ensures that the generative model has access to up-to-date and contextually relevant information, which is particularly useful in applications like question answering and content generation.\n",
      "\n",
      "2. **Reduced Hallucination**: Generative models, especially large language models, can sometimes produce outputs that are plausible-sounding but factually incorrect, a phenomenon known as \"hallucination.\" By incorporating a retrieval step, RAG helps ground the model's responses in real data, thereby reducing the likelihood of hallucinations and increasing the trustworthiness of the output.\n",
      "\n",
      "3. **Scalability and Flexibility**: RAG allows for the integration of vast and diverse external knowledge sources without the need to retrain the generative model. This makes it highly scalable and flexible, as it can easily adapt to new information and domains by simply updating the retrieval database, rather than requiring extensive retraining of the model itself. This is particularly advantageous in dynamic fields where information changes rapidly.\n",
      "   2. RAG, or Retrieval-Augmented Generation, is a technique that combines retrieval-based methods with generative models to enhance AI applications. Here are three key benefits of using RAG:\n",
      "\n",
      "1. **Improved Accuracy and Relevance**: By integrating retrieval mechanisms, RAG can access a vast repository of information to provide contextually relevant data to the generative model. This leads to more accurate and contextually appropriate responses, as the model can ground its outputs in real-world data and facts.\n",
      "\n",
      "2. **Enhanced Knowledge Base**: RAG allows AI systems to dynamically access and update their knowledge base without needing to retrain the entire model. This is particularly useful for applications requiring up-to-date information, as the retrieval component can pull in the latest data from external sources, ensuring that the AI's responses remain current and informed.\n",
      "\n",
      "3. **Efficiency in Handling Large Datasets**: By leveraging retrieval mechanisms, RAG can efficiently handle large datasets without the need to encode all the information directly into the model's parameters. This reduces the computational load and memory requirements, making it feasible to work with extensive and constantly evolving datasets while maintaining high performance.\n",
      "   ğŸ“ˆ Diversity: 3/3 unique\n",
      "   ğŸ“ Avg length: 1302.7 characters\n",
      "\n",
      "ğŸ¯ **Top-p = 0.5**\n",
      "   ğŸ“Š Sample responses:\n",
      "   1. RAG, or Retrieval-Augmented Generation, is a technique that combines retrieval-based methods with generative models to enhance AI applications. Here are three key benefits of using RAG:\n",
      "\n",
      "1. **Improved Accuracy and Relevance**: By integrating retrieval mechanisms, RAG can access a vast repository of information, allowing the model to generate responses that are more accurate and contextually relevant. This is particularly beneficial in applications like question answering, where the model can pull in precise information from a large database to support its generated answers.\n",
      "\n",
      "2. **Reduced Hallucination**: Generative models can sometimes produce plausible-sounding but incorrect or nonsensical information, a phenomenon known as hallucination. RAG mitigates this by grounding the generation process in real, retrieved data, thereby reducing the likelihood of generating false or misleading content.\n",
      "\n",
      "3. **Enhanced Knowledge Base**: RAG allows AI systems to dynamically update their knowledge base without needing to retrain the entire model. By simply updating the underlying retrieval database, the system can access the latest information, making it more adaptable and up-to-date with current knowledge, which is particularly useful in rapidly changing fields.\n",
      "   2. RAG, or Retrieval-Augmented Generation, is a technique that combines retrieval-based methods with generative models to enhance AI applications. Here are three key benefits of using RAG:\n",
      "\n",
      "1. **Improved Accuracy and Relevance**: By integrating retrieval mechanisms, RAG models can access a vast corpus of external data to fetch relevant information, which can then be used to generate more accurate and contextually relevant responses. This is particularly beneficial in applications like question answering, where the model can retrieve specific facts or details from a large database to support its generative capabilities.\n",
      "\n",
      "2. **Enhanced Knowledge Base**: RAG allows AI systems to leverage up-to-date information without the need for frequent retraining. The retrieval component can access the latest data, ensuring that the generative model produces outputs based on current knowledge. This is crucial in dynamic fields where information changes rapidly, such as news or scientific research.\n",
      "\n",
      "3. **Reduced Model Size and Complexity**: By offloading the need to store extensive knowledge within the model itself, RAG can help reduce the size and complexity of the generative model. The retrieval component handles the heavy lifting of accessing and filtering relevant data, allowing the generative model to focus on producing coherent and contextually appropriate outputs. This can lead to more efficient models that require less computational resources while maintaining high performance.\n",
      "   ğŸ“ˆ Diversity: 3/3 unique\n",
      "   ğŸ“ Avg length: 1416.0 characters\n",
      "\n",
      "ğŸ¯ **Top-p = 0.7**\n",
      "   ğŸ“Š Sample responses:\n",
      "   1. RAG, or Retrieval-Augmented Generation, is a technique that combines the strengths of information retrieval and generative models to enhance AI applications. Here are three key benefits of using RAG:\n",
      "\n",
      "1. **Improved Accuracy and Relevance**: By integrating retrieval mechanisms, RAG can access a large corpus of external documents or databases to find relevant information. This allows the generative model to produce more accurate and contextually relevant responses, especially in scenarios where the model's training data alone might not cover specific or up-to-date information.\n",
      "\n",
      "2. **Enhanced Contextual Understanding**: RAG models can dynamically pull in context from external sources, which helps in generating responses that are better informed and more contextually aware. This is particularly beneficial in complex or specialized domains where a deep understanding of the subject matter is required, such as legal, medical, or technical fields.\n",
      "\n",
      "3. **Efficiency and Scalability**: By leveraging retrieval systems, RAG models can efficiently handle large-scale knowledge bases without the need to embed all possible knowledge directly within the generative model. This reduces the computational load and allows the system to scale more effectively, as it can access and utilize vast amounts of information without needing to retrain the model frequently.\n",
      "   2. RAG, or Retrieval-Augmented Generation, is a technique that combines retrieval-based methods with generative models to enhance the performance of AI applications. Here are three key benefits of using RAG:\n",
      "\n",
      "1. **Improved Accuracy and Relevance**: By integrating a retrieval component, RAG can access a vast repository of external knowledge, which helps in generating more accurate and contextually relevant responses. This is particularly beneficial in tasks like question answering, where the model can retrieve specific information from a database or document corpus to support its generative output.\n",
      "\n",
      "2. **Handling of Rare or Out-of-Distribution Queries**: RAG is effective in dealing with rare or out-of-distribution queries that a generative model alone might struggle with. The retrieval mechanism can pull in pertinent information from a diverse set of sources, ensuring that even less common topics are addressed with up-to-date and precise information.\n",
      "\n",
      "3. **Efficiency and Scalability**: By leveraging retrieval methods, RAG can reduce the computational burden on the generative model. Instead of relying solely on the generative model to produce responses from scratch, RAG uses retrieved documents or data as a foundation, which can lead to more efficient processing and scalability, especially when dealing with large-scale data or complex queries.\n",
      "   ğŸ“ˆ Diversity: 3/3 unique\n",
      "   ğŸ“ Avg length: 1339.0 characters\n",
      "\n",
      "ğŸ¯ **Top-p = 0.9**\n",
      "   ğŸ“Š Sample responses:\n",
      "   1. RAG, or Retrieval-Augmented Generation, is an approach that combines retrieval-based and generation-based techniques in AI applications. Here are three key benefits of using RAG:\n",
      "\n",
      "1. **Enhanced Accuracy and Relevance**: RAG leverages a retrieval mechanism to access a vast amount of external knowledge, which it then uses to inform the generation process. This leads to more accurate and contextually relevant outputs, as the model can incorporate specific and up-to-date information from a wide range of sources rather than relying solely on pre-trained data.\n",
      "\n",
      "2. **Improved Generalization**: By integrating real-time data retrieval, RAG systems can generalize better across different topics and domains. This is particularly beneficial for applications that require up-to-date information or need to handle a diverse set of queries. The retrieval component allows the model to adapt to new or niche topics without needing extensive retraining.\n",
      "\n",
      "3. **Resource Efficiency**: RAG systems can be more resource-efficient compared to models that rely solely on massive datasets for training. By dynamically retrieving relevant information as needed, RAG reduces the need for storing and processing enormous datasets, which can lead to cost savings in terms of storage and computational resources. This efficiency also enables quicker adaptation to new information or changes in the domain of interest.\n",
      "   2. RAG, or Retrieval-Augmented Generation, is an approach in AI that combines retrieval-based methods with generative models to enhance performance and accuracy. Here are three key benefits of using RAG in AI applications:\n",
      "\n",
      "1. **Improved Accuracy and Relevance**: By integrating retrieval mechanisms, RAG allows models to access and incorporate up-to-date and contextually relevant information from a large database or corpus. This leads to more accurate and contextually appropriate responses, especially in dynamic domains where data changes frequently.\n",
      "\n",
      "2. **Reduced Hallucination**: One of the challenges with purely generative models is their tendency to \"hallucinate,\" or generate plausible-sounding but incorrect or nonsensical information. By grounding responses in retrieved data, RAG reduces the likelihood of hallucinations, leading to more reliable and factually correct outputs.\n",
      "\n",
      "3. **Enhanced Flexibility and Adaptability**: RAG models can be easily updated and adapted to new domains or topics by simply updating the underlying retrieval database, without the need for retraining the entire generative model. This flexibility is particularly beneficial for applications that require frequent updates or customization to specific user needs.\n",
      "   ğŸ“ˆ Diversity: 3/3 unique\n",
      "   ğŸ“ Avg length: 1366.3 characters\n",
      "\n",
      "ğŸ¯ **Top-p = 1.0**\n",
      "   ğŸ“Š Sample responses:\n",
      "   1. RAG, or Retrieval-Augmented Generation, is a technique that combines the strengths of information retrieval and generative models. Here are three key benefits of using RAG in AI applications:\n",
      "\n",
      "1. **Enhanced Knowledge Utilization**: RAG allows AI systems to access and incorporate external knowledge sources in real-time, which significantly enhances the system's ability to provide accurate and contextually relevant responses. This is particularly beneficial in applications where up-to-date or specialized information is crucial, such as customer support or medical diagnosis.\n",
      "\n",
      "2. **Improved Response Accuracy**: By retrieving relevant documents or data points before generating a response, RAG helps in grounding the generative model's output in factual information. This reduces the likelihood of generating incorrect or nonsensical information, thereby improving the overall accuracy and reliability of AI-generated content.\n",
      "\n",
      "3. **Scalability and Flexibility**: RAG frameworks can be tailored to access various types of databases and information sources, which makes them highly scalable and flexible. This adaptability is advantageous for developing AI applications across different domains and industries, as it allows for easy integration of domain-specific knowledge without the need for extensive retraining of the model.\n",
      "   2. RAG, or Retrieval-Augmented Generation, is a framework that combines retrieval-based and generation-based approaches in AI applications. Here are three key benefits of using RAG:\n",
      "\n",
      "1. **Enhanced Knowledge Utilization**: RAG leverages large external knowledge bases or datasets to retrieve relevant information and incorporate it into the generation process. This allows the model to access up-to-date and expansive information beyond its training data, thereby improving the accuracy and relevance of its outputs, especially in dynamic or specialized domains.\n",
      "\n",
      "2. **Improved Contextual Understanding**: By combining retrieval mechanisms with generative capabilities, RAG can better understand and incorporate context from relevant documents or data sources. This leads to more coherent and contextually appropriate responses, as the model can ground its answers in specific pieces of retrieved information.\n",
      "\n",
      "3. **Reduced Hallucination**: One common issue with purely generative models is their tendency to \"hallucinate\" or produce factually incorrect information. RAG mitigates this by anchoring the generation process in retrieved, real-world data, which helps ensure that the generated content is more accurate and reliable. This is particularly valuable in applications requiring high factual correctness, such as customer support or scientific research.\n",
      "   ğŸ“ˆ Diversity: 3/3 unique\n",
      "   ğŸ“ Avg length: 1310.0 characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def explore_top_p_effects():\n",
    "    \"\"\"\n",
    "    Comprehensive exploration of top-p parameter effects.\n",
    "    \"\"\"\n",
    "    print(\"ğŸ¯ Exploring Top-p (Nucleus Sampling) Effects\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Test prompt that benefits from controlled diversity\n",
    "    test_prompt = \"List 3 key benefits of using RAG in AI applications.\"\n",
    "    \n",
    "    # Top-p values to test\n",
    "    top_p_values = [0.1, 0.3, 0.5, 0.7, 0.9, 1.0]\n",
    "    \n",
    "    # Create parameter sets with fixed temperature\n",
    "    parameter_sets = [{'temperature': 0.7, 'top_p': p} for p in top_p_values]\n",
    "    \n",
    "    # Run experiments\n",
    "    results = explorer.batch_experiment(\n",
    "        prompt=test_prompt,\n",
    "        parameter_sets=parameter_sets,\n",
    "        runs_per_set=3\n",
    "    )\n",
    "    \n",
    "    return results\n",
    "\n",
    "def analyze_top_p_results(results):\n",
    "    \"\"\"\n",
    "    Analyze top-p experiment results.\n",
    "    \"\"\"\n",
    "    print(\"\\nğŸ“Š Top-p Analysis Results\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Group by top_p value\n",
    "    top_p_groups = {}\n",
    "    for result in results:\n",
    "        p = result['parameters']['top_p']\n",
    "        if p not in top_p_groups:\n",
    "            top_p_groups[p] = []\n",
    "        top_p_groups[p].append(result)\n",
    "    \n",
    "    # Analyze each group\n",
    "    for p in sorted(top_p_groups.keys()):\n",
    "        group = top_p_groups[p]\n",
    "        responses = [r['response'] for r in group]\n",
    "        \n",
    "        print(f\"\\nğŸ¯ **Top-p = {p}**\")\n",
    "        \n",
    "        # Show sample responses\n",
    "        print(f\"   ğŸ“Š Sample responses:\")\n",
    "        for i, response in enumerate(responses[:2]):  # Show first 2\n",
    "            preview = response\n",
    "            print(f\"   {i+1}. {preview}\")\n",
    "        \n",
    "        # Calculate diversity metrics\n",
    "        unique_responses = len(set(responses))\n",
    "        avg_length = np.mean([len(r) for r in responses])\n",
    "        \n",
    "        print(f\"   ğŸ“ˆ Diversity: {unique_responses}/{len(responses)} unique\")\n",
    "        print(f\"   ğŸ“ Avg length: {avg_length:.1f} characters\")\n",
    "    \n",
    "    return top_p_groups\n",
    "\n",
    "# Run top-p exploration\n",
    "top_p_results = explore_top_p_effects()\n",
    "top_p_analysis = analyze_top_p_results(top_p_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ¯ Top-p Key Insights:\n",
    "\n",
    "- **Top-p = 0.1**: Very focused, limited vocabulary, consistent style\n",
    "- **Top-p = 0.3-0.5**: Balanced focus and variety, good for factual content\n",
    "- **Top-p = 0.7-0.9**: Good diversity while maintaining coherence\n",
    "- **Top-p = 1.0**: Maximum diversity, all tokens possible\n",
    "\n",
    "**For RAG Systems:**\n",
    "- **Factual responses**: Use 0.3-0.7 for focused, accurate answers\n",
    "- **Creative content**: Use 0.7-0.9 for varied, engaging responses\n",
    "- **Consistent formatting**: Use 0.1-0.5 for predictable structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2-3'></a>\n",
    "### 2.3 Top-k Sampling\n",
    "\n",
    "**Top-k** limits token selection to the k most probable tokens, providing a different approach to controlling diversity.\n",
    "\n",
    "**What we're doing:** Comparing top-k with top-p to understand when each approach is most effective for RAG applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Comparing Sampling Methods\n",
      "========================================\n",
      "\n",
      "ğŸ§ª Testing: Temperature Only\n",
      "   Parameters: {'temperature': 0.7}\n",
      "   ğŸ“ Response: Vector databases play a crucial role in Retrieval-Augmented Generation (RAG) systems, which are used to enhance the capabilities of models like language models by providing them with access to external knowledge bases. Here's how vector databases function within RAG systems:\n",
      "\n",
      "### 1. **Embedding Generation:**\n",
      "   - **Text to Vector Conversion:** The process begins by converting text data into vector representations, known as embeddings. This is typically done using pre-trained language models or neural networks that map text to a high-dimensional vector space.\n",
      "   - **Contextual Understanding:** These embeddings capture the semantic meaning of the text, allowing for a deeper understanding of the context and relationships between different pieces of information.\n",
      "\n",
      "### 2. **Storage in Vector Database:**\n",
      "   - **Efficient Storage:** The generated embeddings are stored in a vector database. Unlike traditional databases that store data in tabular form, vector databases are optimized for storing and retrieving high-dimensional vectors.\n",
      "   - **Indexing:** Vector databases use advanced indexing techniques such as approximate nearest neighbor (ANN) algorithms to efficiently manage and search large volumes of vector data.\n",
      "\n",
      "### 3. **Retrieval Process:**\n",
      "   - **Query Embedding:** When a query is made to the RAG system, it is first converted into an embedding using the same or similar model used for the data embeddings.\n",
      "   - **Similarity Search:** The vector database is then queried to find vectors (and hence the corresponding text data) that are most similar to the query embedding. This is achieved through similarity measures like cosine similarity or Euclidean distance.\n",
      "\n",
      "### 4. **Augmentation:**\n",
      "   - **Information Retrieval:** The retrieved pieces of information (text data) are considered relevant to the query and are used to augment the language model's response generation process.\n",
      "   - **Contextual Response:** The language model uses this additional context to generate responses that are more accurate, informative, and contextually relevant.\n",
      "\n",
      "### 5. **Applications:**\n",
      "   - **Question Answering:** Vector databases in RAG systems enable efficient retrieval of relevant documents or passages that can be used to answer user queries.\n",
      "   - **Recommendation Systems:** They can enhance recommendation systems by understanding user preferences and retrieving similar content.\n",
      "   - **Knowledge Management:** Organizations use them to manage and access large corpora of knowledge efficiently.\n",
      "\n",
      "### Key Advantages:\n",
      "- **Scalability:** Vector databases are designed to handle large-scale data, making them suitable for applications with vast amounts of information.\n",
      "- **Performance:** They offer fast\n",
      "   ğŸ“Š Length: 379 words\n",
      "   â±ï¸ Time: 7.64s\n",
      "\n",
      "ğŸ§ª Testing: Top-p Focused\n",
      "   Parameters: {'temperature': 0.7, 'top_p': 0.3}\n",
      "   ğŸ“ Response: Vector databases play a crucial role in Retrieval-Augmented Generation (RAG) systems by efficiently storing and retrieving high-dimensional vector representations of data. These systems are designed to enhance the capabilities of generative AI models by providing them with relevant context or information retrieved from a large corpus of data. Here's how vector databases work within RAG systems:\n",
      "\n",
      "1. **Data Representation**: In a RAG system, data (such as text, images, or other media) is first converted into vector representations using embedding models. These models, often based on neural networks, transform data into fixed-size vectors in a high-dimensional space. The vectors capture semantic information, meaning that similar pieces of data are represented by vectors that are close to each other in this space.\n",
      "\n",
      "2. **Storage**: Once the data is converted into vectors, these vectors are stored in a vector database. Unlike traditional databases that store structured data, vector databases are optimized for handling high-dimensional vectors. They are designed to efficiently store, index, and retrieve these vectors.\n",
      "\n",
      "3. **Indexing**: Vector databases use specialized indexing techniques to facilitate fast similarity searches. Common indexing methods include approximate nearest neighbor (ANN) algorithms like HNSW (Hierarchical Navigable Small World) or FAISS (Facebook AI Similarity Search). These methods allow the database to quickly find vectors that are similar to a given query vector.\n",
      "\n",
      "4. **Retrieval**: When a RAG system needs to generate a response or perform a task, it first retrieves relevant information from the vector database. This is done by converting the query (e.g., a user question) into a vector using the same embedding model. The vector database then performs a similarity search to find vectors that are closest to the query vector, effectively retrieving the most relevant pieces of data.\n",
      "\n",
      "5. **Augmentation**: The retrieved data is then used to augment the input to a generative model. For example, in a question-answering system, the retrieved documents or text snippets provide context that helps the generative model produce more accurate and contextually relevant answers.\n",
      "\n",
      "6. **Feedback Loop**: Some RAG systems incorporate a feedback loop where the performance of the retrieval and generation components is continuously evaluated. This feedback can be used to fine-tune the embedding models or adjust the retrieval strategies to improve the overall system performance.\n",
      "\n",
      "In summary, vector databases are essential in RAG systems for efficiently managing and retrieving high-dimensional vector representations of data. They enable the system to provide relevant context to generative\n",
      "   ğŸ“Š Length: 397 words\n",
      "   â±ï¸ Time: 10.75s\n",
      "\n",
      "ğŸ§ª Testing: Top-p Balanced\n",
      "   Parameters: {'temperature': 0.7, 'top_p': 0.7}\n",
      "   ğŸ“ Response: Vector databases play a crucial role in Retrieval-Augmented Generation (RAG) systems by efficiently storing and retrieving high-dimensional vector representations of data. These vector representations are typically generated using machine learning models, such as transformers or other embedding models, which convert complex data types (like text, images, or audio) into numerical vectors. Here's how vector databases work within RAG systems:\n",
      "\n",
      "1. **Data Embedding**: The first step in a RAG system is to convert the input data into vector embeddings. This is done using a pre-trained model that processes the data (e.g., text) and outputs a fixed-size vector. Each vector captures the semantic meaning of the data, allowing similar pieces of data to have similar vector representations.\n",
      "\n",
      "2. **Vector Storage**: Once the data is converted into vectors, these vectors are stored in a vector database. Unlike traditional databases that store structured data in tables, vector databases are optimized for storing and querying high-dimensional vectors. They often use specialized data structures like HNSW (Hierarchical Navigable Small World) graphs, Annoy (Approximate Nearest Neighbors Oh Yeah), or FAISS (Facebook AI Similarity Search) to facilitate efficient storage and retrieval.\n",
      "\n",
      "3. **Similarity Search**: A key feature of vector databases is their ability to perform similarity searches. When a query is made, it is also converted into a vector using the same embedding model. The vector database then retrieves vectors from its storage that are closest to the query vector based on a similarity metric, typically cosine similarity or Euclidean distance. This process is often referred to as nearest neighbor search.\n",
      "\n",
      "4. **Retrieval-Augmented Generation**: In a RAG system, the retrieved vectors are used to augment the input to a generative model. For example, in a question-answering system, the retrieved vectors might correspond to relevant documents or passages. These are then used to provide context or additional information to a generative model (like GPT or BERT) that constructs the final output or response.\n",
      "\n",
      "5. **Efficiency and Scalability**: Vector databases are designed to handle large-scale data efficiently. They support fast retrieval even with millions or billions of vectors, making them suitable for applications that require real-time responses. They also often support distributed architectures to scale with growing data volumes.\n",
      "\n",
      "6. **Applications**: Vector databases in RAG systems are widely used in applications like search engines, recommendation systems, question-answering systems, and any domain where understanding and generating contextually relevant\n",
      "   ğŸ“Š Length: 387 words\n",
      "   â±ï¸ Time: 7.99s\n",
      "\n",
      "ğŸ§ª Testing: Top-p Diverse\n",
      "   Parameters: {'temperature': 0.7, 'top_p': 0.9}\n",
      "   ğŸ“ Response: Vector databases play a crucial role in Retrieval-Augmented Generation (RAG) systems by facilitating efficient storage, retrieval, and manipulation of high-dimensional vector representations of data. RAG systems combine traditional information retrieval with generative models to improve the quality and relevance of generated content. Hereâ€™s how vector databases work within these systems:\n",
      "\n",
      "1. **Vectorization of Data**: \n",
      "   - In a RAG system, data (such as text, images, or other content types) is first converted into vector representations using machine learning models, often leveraging embeddings from pre-trained neural networks like BERT, GPT, or others.\n",
      "   - These embeddings capture semantic meanings and relationships, allowing the system to understand and process the data more effectively.\n",
      "\n",
      "2. **Storage in Vector Databases**:\n",
      "   - Once data is vectorized, the resulting high-dimensional vectors are stored in a vector database. Unlike traditional databases that store structured data in tables, vector databases are optimized to handle large volumes of unstructured data in the form of vectors.\n",
      "   - These databases are designed to support efficient similarity search operations, which are essential for retrieving relevant information based on vector proximity.\n",
      "\n",
      "3. **Similarity Search and Retrieval**:\n",
      "   - A core functionality of vector databases is performing similarity searches, which involve finding vectors in the database that are closest to a given query vector. This is typically done using distance metrics like cosine similarity, Euclidean distance, or inner product.\n",
      "   - When a query is made, the system generates a vector representation of the query and searches the database for vectors that are most similar to it. This helps in retrieving relevant documents or information that can be used to augment the generative process.\n",
      "\n",
      "4. **Integration with Generative Models**:\n",
      "   - The retrieved vectors or documents can then be fed into generative models like GPT to produce more contextually relevant and accurate outputs.\n",
      "   - This integration allows the generative model to leverage both the deep understanding encoded in the vectors and the specific, retrieved information to generate responses that are not only coherent but also enriched with real-time, context-specific data.\n",
      "\n",
      "5. **Scalability and Performance**:\n",
      "   - Vector databases are optimized for handling large-scale data and performing fast similarity searches, often using techniques like Approximate Nearest Neighbor (ANN) search to balance between accuracy and performance.\n",
      "   - They support distributed architectures to manage large datasets efficiently, ensuring that RAG systems can operate in real-time applications.\n",
      "\n",
      "In summary, vector databases are essential in RAG systems for managing and\n",
      "   ğŸ“Š Length: 395 words\n",
      "   â±ï¸ Time: 9.73s\n",
      "\n",
      "ğŸ§ª Testing: Low Temp + High Top-p\n",
      "   Parameters: {'temperature': 0.3, 'top_p': 0.9}\n",
      "   ğŸ“ Response: Retrieval-Augmented Generation (RAG) systems are a type of AI model that combine the strengths of information retrieval and generative models to produce more accurate and contextually relevant responses. Vector databases play a crucial role in these systems by efficiently managing and retrieving high-dimensional data representations. Here's how vector databases work within RAG systems:\n",
      "\n",
      "1. **Embedding Generation**: \n",
      "   - The first step in a RAG system involves converting textual data into vector representations, known as embeddings. This is typically done using models like BERT, GPT, or other transformer-based architectures. These embeddings capture semantic meanings and relationships between words and phrases in a high-dimensional space.\n",
      "\n",
      "2. **Storing Embeddings**:\n",
      "   - Once the data is converted into embeddings, these vectors are stored in a vector database. Unlike traditional databases that store data in tabular formats, vector databases are optimized for storing and querying high-dimensional vectors. Examples of vector databases include Pinecone, Weaviate, and FAISS.\n",
      "\n",
      "3. **Efficient Similarity Search**:\n",
      "   - Vector databases are designed to perform fast similarity searches. When a query is made, it is also converted into an embedding. The vector database then performs a nearest neighbor search to find vectors (and thus the original data) that are most similar to the query vector. This is typically done using distance metrics like cosine similarity or Euclidean distance.\n",
      "\n",
      "4. **Retrieval of Relevant Information**:\n",
      "   - The nearest neighbor search results in a set of documents or data points that are most relevant to the query. These retrieved documents provide context and factual grounding for the generative model.\n",
      "\n",
      "5. **Augmenting Generation**:\n",
      "   - The generative model uses the retrieved documents to produce a response that is both contextually relevant and factually accurate. By grounding its output in the retrieved information, the model can generate responses that are more aligned with the user's query and the available data.\n",
      "\n",
      "6. **Continuous Learning and Updating**:\n",
      "   - Vector databases in RAG systems can be continuously updated with new data, allowing the system to learn and adapt over time. This ensures that the system remains relevant and up-to-date with the latest information.\n",
      "\n",
      "In summary, vector databases are essential in RAG systems for efficiently managing and retrieving high-dimensional data. They enable the system to find and use relevant information quickly, enhancing the quality and accuracy of the generated responses.\n",
      "   ğŸ“Š Length: 375 words\n",
      "   â±ï¸ Time: 12.69s\n",
      "\n",
      "ğŸ§ª Testing: High Temp + Low Top-p\n",
      "   Parameters: {'temperature': 1.2, 'top_p': 0.3}\n",
      "   ğŸ“ Response: Vector databases play a crucial role in Retrieval-Augmented Generation (RAG) systems by efficiently storing and retrieving high-dimensional vector representations of data. Here's a breakdown of how they work within RAG systems:\n",
      "\n",
      "1. **Vector Representation**: In RAG systems, data such as text, images, or other content types are first converted into vector representations using machine learning models, often neural networks. For text, this might involve using models like BERT, GPT, or other transformers to generate embeddings that capture semantic meaning.\n",
      "\n",
      "2. **Storage**: These vector representations are stored in a vector database. Unlike traditional databases that store structured data in tables, vector databases are optimized for handling high-dimensional vectors. They are designed to efficiently store, index, and retrieve these vectors.\n",
      "\n",
      "3. **Indexing**: Vector databases use specialized indexing techniques to enable fast similarity searches. Common methods include approximate nearest neighbor (ANN) algorithms like HNSW (Hierarchical Navigable Small World), FAISS (Facebook AI Similarity Search), or Annoy. These techniques allow the database to quickly find vectors that are similar to a given query vector.\n",
      "\n",
      "4. **Retrieval**: When a RAG system needs to retrieve relevant information, it converts the query (e.g., a question or a prompt) into a vector using the same or a similar embedding model. This query vector is then used to search the vector database for the most similar vectors, which correspond to the most relevant pieces of data.\n",
      "\n",
      "5. **Augmentation**: The retrieved data, which could be text passages, documents, or other content, is then used to augment the input to a generative model. This model, often a large language model, uses the retrieved information to generate more accurate and contextually relevant responses.\n",
      "\n",
      "6. **Applications**: Vector databases in RAG systems are particularly useful in applications like question answering, recommendation systems, and any scenario where contextually relevant information needs to be retrieved and used to enhance the output of generative models.\n",
      "\n",
      "Overall, vector databases enable RAG systems to efficiently handle and retrieve large volumes of unstructured data, providing the necessary context to improve the quality and relevance of generated content.\n",
      "   ğŸ“Š Length: 336 words\n",
      "   â±ï¸ Time: 9.87s\n",
      "\n",
      "ğŸ“ˆ Sampling Method Analysis\n",
      "===================================\n",
      "               Method  Word Count Response Time Total Cost Coherence Score\n",
      "     Temperature Only         379         7.64s    $0.0050            High\n",
      "        Top-p Focused         397        10.75s    $0.0050            High\n",
      "       Top-p Balanced         387         7.99s    $0.0050            High\n",
      "        Top-p Diverse         395         9.73s    $0.0050            High\n",
      "Low Temp + High Top-p         375        12.69s    $0.0048            High\n",
      "High Temp + Low Top-p         336         9.87s    $0.0044            High\n"
     ]
    }
   ],
   "source": [
    "def compare_sampling_methods():\n",
    "    \"\"\"\n",
    "    Compare different sampling methods: top-k vs top-p vs temperature-only.\n",
    "    \"\"\"\n",
    "    print(\"ğŸ”„ Comparing Sampling Methods\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Test prompt for comparison\n",
    "    test_prompt = \"Explain how vector databases work in RAG systems.\"\n",
    "    \n",
    "    # Different sampling approaches\n",
    "    sampling_configs = [\n",
    "        {'name': 'Temperature Only', 'params': {'temperature': 0.7}},\n",
    "        {'name': 'Top-p Focused', 'params': {'temperature': 0.7, 'top_p': 0.3}},\n",
    "        {'name': 'Top-p Balanced', 'params': {'temperature': 0.7, 'top_p': 0.7}},\n",
    "        {'name': 'Top-p Diverse', 'params': {'temperature': 0.7, 'top_p': 0.9}},\n",
    "        {'name': 'Low Temp + High Top-p', 'params': {'temperature': 0.3, 'top_p': 0.9}},\n",
    "        {'name': 'High Temp + Low Top-p', 'params': {'temperature': 1.2, 'top_p': 0.3}}\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for config in sampling_configs:\n",
    "        print(f\"\\nğŸ§ª Testing: {config['name']}\")\n",
    "        print(f\"   Parameters: {config['params']}\")\n",
    "        \n",
    "        try:\n",
    "            result = explorer.generate_response(\n",
    "                prompt=test_prompt,\n",
    "                **config['params']\n",
    "            )\n",
    "            result['config_name'] = config['name']\n",
    "            results.append(result)\n",
    "            \n",
    "            # Show preview\n",
    "            preview = result['response']\n",
    "            print(f\"   ğŸ“ Response: {preview}\")\n",
    "            print(f\"   ğŸ“Š Length: {result['metrics']['word_count']} words\")\n",
    "            print(f\"   â±ï¸ Time: {result['metrics']['response_time']:.2f}s\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ Error: {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def analyze_sampling_comparison(results):\n",
    "    \"\"\"\n",
    "    Analyze the comparison results to identify optimal configurations.\n",
    "    \"\"\"\n",
    "    print(\"\\nğŸ“ˆ Sampling Method Analysis\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    # Create comparison table\n",
    "    comparison_data = []\n",
    "    \n",
    "    for result in results:\n",
    "        comparison_data.append({\n",
    "            'Method': result['config_name'],\n",
    "            'Word Count': result['metrics']['word_count'],\n",
    "            'Response Time': f\"{result['metrics']['response_time']:.2f}s\",\n",
    "            'Total Cost': f\"${result['metrics']['total_cost']:.4f}\",\n",
    "            'Coherence Score': 'High' if 'vector' in result['response'].lower() and 'database' in result['response'].lower() else 'Medium'\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(comparison_data)\n",
    "    print(df.to_string(index=False))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Run sampling comparison\n",
    "sampling_results = compare_sampling_methods()\n",
    "sampling_analysis = analyze_sampling_comparison(sampling_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ”„ Sampling Method Key Insights:\n",
    "\n",
    "- **Temperature Only**: Simple but less predictable diversity control\n",
    "- **Top-p + Temperature**: Best balance of control and creativity\n",
    "- **Low Temp + High Top-p**: Consistent quality with controlled variety\n",
    "- **High Temp + Low Top-p**: Creative but focused vocabulary\n",
    "\n",
    "**Recommended Combinations for RAG:**\n",
    "1. **Factual Q&A**: `temperature=0.3, top_p=0.7`\n",
    "2. **Explanatory Content**: `temperature=0.7, top_p=0.8`\n",
    "3. **Creative Tasks**: `temperature=0.9, top_p=0.9`\n",
    "4. **Consistent Formatting**: `temperature=0.2, top_p=0.5`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5'></a>\n",
    "## 5 - Production Optimization\n",
    "\n",
    "---\n",
    "\n",
    "<a id='5-1'></a>\n",
    "### 5.1 Performance vs Quality Trade-offs\n",
    "\n",
    "**What we're doing:** Understanding the critical balance between response quality, speed, and cost in production RAG systems. We'll develop frameworks for making informed parameter choices based on business requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’° Cost Optimization Strategies for Production RAG\n",
      "====================================================\n",
      "\n",
      "ğŸ§ª **Testing: Baseline (No Optimization)**\n",
      "   ğŸ’¡ Strategy: Standard parameters without optimization\n",
      "   âš™ï¸ Parameters: {'temperature': 0.7, 'top_p': 0.9, 'max_tokens': 500}\n",
      "   ğŸ“Š Results:\n",
      "      â€¢ Total Cost: $0.0051\n",
      "      â€¢ Words Generated: 387\n",
      "      â€¢ Cost per Word: $0.000013\n",
      "      â€¢ Quality Score: 0.80/1.0\n",
      "      â€¢ Cost Efficiency: 157.02\n",
      "      â€¢ Response Preview: Certainly! RAG, which stands for Retrieval-Augmented Generation, is a technique used to enhance the performance and reli...\n",
      "\n",
      "ğŸ§ª **Testing: Token Limiting**\n",
      "   ğŸ’¡ Strategy: Reduce max_tokens while maintaining quality\n",
      "   âš™ï¸ Parameters: {'temperature': 0.7, 'top_p': 0.9, 'max_tokens': 200}\n",
      "   ğŸ“Š Results:\n",
      "      â€¢ Total Cost: $0.0021\n",
      "      â€¢ Words Generated: 144\n",
      "      â€¢ Cost per Word: $0.000015\n",
      "      â€¢ Quality Score: 0.80/1.0\n",
      "      â€¢ Cost Efficiency: 382.78\n",
      "      â€¢ Response Preview: RAG (Retrieval-Augmented Generation) systems play a crucial role in making large language model (LLM)-powered applicatio...\n",
      "\n",
      "ğŸ§ª **Testing: Temperature Reduction**\n",
      "   ğŸ’¡ Strategy: Lower temperature for more direct responses\n",
      "   âš™ï¸ Parameters: {'temperature': 0.3, 'top_p': 0.9, 'max_tokens': 300}\n",
      "   ğŸ“Š Results:\n",
      "      â€¢ Total Cost: $0.0009\n",
      "      â€¢ Words Generated: 66\n",
      "      â€¢ Cost per Word: $0.000014\n",
      "      â€¢ Quality Score: 0.80/1.0\n",
      "      â€¢ Cost Efficiency: 851.06\n",
      "      â€¢ Response Preview: RAG (Retrieval-Augmented Generation) systems enhance LLM-powered applications by integrating external knowledge retrieva...\n",
      "\n",
      "ğŸ§ª **Testing: Structured Response**\n",
      "   ğŸ’¡ Strategy: Guide LLM to produce structured, efficient responses\n",
      "   âš™ï¸ Parameters: {'temperature': 0.4, 'top_p': 0.7, 'max_tokens': 250}\n",
      "   ğŸ“Š Results:\n",
      "      â€¢ Total Cost: $0.0016\n",
      "      â€¢ Words Generated: 114\n",
      "      â€¢ Cost per Word: $0.000014\n",
      "      â€¢ Quality Score: 0.60/1.0\n",
      "      â€¢ Cost Efficiency: 385.23\n",
      "      â€¢ Response Preview: - RAG (Retrieval-Augmented Generation) systems enhance LLM-powered applications by integrating retrieval mechanisms that...\n",
      "\n",
      "ğŸ§ª **Testing: Smart Prompting**\n",
      "   ğŸ’¡ Strategy: More specific prompts to reduce unnecessary generation\n",
      "   âš™ï¸ Parameters: {'temperature': 0.5, 'top_p': 0.8, 'max_tokens': 300}\n",
      "   ğŸ“Š Results:\n",
      "      â€¢ Total Cost: $0.0031\n",
      "      â€¢ Words Generated: 236\n",
      "      â€¢ Cost per Word: $0.000013\n",
      "      â€¢ Quality Score: 1.00/1.0\n",
      "      â€¢ Cost Efficiency: 322.06\n",
      "      â€¢ Response Preview: 1. **Enhanced Decision-Making**: RAG (Red, Amber, Green) systems provide a clear and immediate visual representation of ...\n",
      "\n",
      "ğŸ“ˆ Cost Optimization Analysis\n",
      "==============================\n",
      "\n",
      "ğŸ† **Cost Efficiency Ranking** (Quality/Cost Ratio)\n",
      "-------------------------------------------------------\n",
      "1. **Temperature Reduction**\n",
      "   ğŸ’° Cost: $0.0009\n",
      "   ğŸ“ Words: 66\n",
      "   ğŸ¯ Quality: 0.80\n",
      "   âš¡ Efficiency: 851.06\n",
      "\n",
      "2. **Structured Response**\n",
      "   ğŸ’° Cost: $0.0016\n",
      "   ğŸ“ Words: 114\n",
      "   ğŸ¯ Quality: 0.60\n",
      "   âš¡ Efficiency: 385.23\n",
      "\n",
      "3. **Token Limiting**\n",
      "   ğŸ’° Cost: $0.0021\n",
      "   ğŸ“ Words: 144\n",
      "   ğŸ¯ Quality: 0.80\n",
      "   âš¡ Efficiency: 382.78\n",
      "\n",
      "4. **Smart Prompting**\n",
      "   ğŸ’° Cost: $0.0031\n",
      "   ğŸ“ Words: 236\n",
      "   ğŸ¯ Quality: 1.00\n",
      "   âš¡ Efficiency: 322.06\n",
      "\n",
      "5. **Baseline (No Optimization)**\n",
      "   ğŸ’° Cost: $0.0051\n",
      "   ğŸ“ Words: 387\n",
      "   ğŸ¯ Quality: 0.80\n",
      "   âš¡ Efficiency: 157.02\n",
      "\n",
      "ğŸ’¡ **Cost Savings Analysis**\n",
      "----------------------------\n",
      "**Structured Response:**\n",
      "   ğŸ’° Cost Savings: $0.0035 (69.4%)\n",
      "   ğŸ“Š Quality Change: -0.20\n",
      "   ğŸ¯ Recommended for: High-volume applications\n",
      "\n",
      "**Token Limiting:**\n",
      "   ğŸ’° Cost Savings: $0.0030 (59.0%)\n",
      "   ğŸ“Š Quality Change: +0.00\n",
      "   ğŸ¯ Recommended for: High-volume applications\n",
      "\n",
      "**Smart Prompting:**\n",
      "   ğŸ’° Cost Savings: $0.0020 (39.1%)\n",
      "   ğŸ“Š Quality Change: +0.20\n",
      "   ğŸ¯ Recommended for: High-volume applications\n",
      "\n",
      "**Baseline (No Optimization):**\n",
      "   ğŸ’° Cost Savings: $0.0000 (0.0%)\n",
      "   ğŸ“Š Quality Change: +0.00\n",
      "   ğŸ¯ Recommended for: Quality-sensitive applications\n",
      "\n",
      "ğŸ› ï¸ **Production Cost Optimization Framework**\n",
      "==============================================\n",
      "\n",
      "**Immediate Actions:**\n",
      "1. Set appropriate max_tokens limits based on use case requirements\n",
      "2. Use structured prompts to guide efficient response generation\n",
      "3. Implement temperature reduction for straightforward queries\n",
      "4. Add response format specifications to reduce unnecessary text\n",
      "\n",
      "**Advanced Strategies:**\n",
      "1. Implement query complexity analysis to adjust parameters dynamically\n",
      "2. Use caching for frequently asked questions to avoid repeated API calls\n",
      "3. Implement response streaming to improve perceived performance\n",
      "4. Create parameter profiles for different user tiers (free vs premium)\n",
      "\n",
      "**Monitoring Metrics:**\n",
      "1. Cost per successful query\n",
      "2. Average response quality score\n",
      "3. Token utilization efficiency\n",
      "4. User satisfaction vs cost trade-off ratio\n",
      "\n",
      "ğŸ“‹ **Implementation Checklist**\n",
      "-------------------------\n",
      "â–¡ Analyze current parameter usage and costs\n",
      "â–¡ Define quality thresholds for different use cases\n",
      "â–¡ Implement A/B testing for parameter optimization\n",
      "â–¡ Set up cost monitoring and alerting\n",
      "â–¡ Create parameter profiles for different scenarios\n",
      "â–¡ Implement automatic parameter adjustment based on load\n",
      "â–¡ Monitor user satisfaction metrics alongside costs\n",
      "â–¡ Regular review and optimization of parameter settings\n"
     ]
    }
   ],
   "source": [
    "def implement_cost_optimization_strategies():\n",
    "    \"\"\"\n",
    "    Implement and test various cost optimization strategies for production RAG.\n",
    "    \"\"\"\n",
    "    print(\"ğŸ’° Cost Optimization Strategies for Production RAG\")\n",
    "    print(\"=\" * 52)\n",
    "    \n",
    "    # Test prompt that could generate long responses\n",
    "    base_prompt = \"Can you explain the role of RAG systems in making LLM-powered applications production-ready?\"\n",
    "    \n",
    "    optimization_strategies = [\n",
    "        {\n",
    "            'name': 'Baseline (No Optimization)',\n",
    "            'strategy': 'Standard parameters without optimization',\n",
    "            'params': {'temperature': 0.7, 'top_p': 0.9, 'max_tokens': 500},\n",
    "            'system_message': \"Provide a comprehensive response to the user's question.\"\n",
    "        },\n",
    "        {\n",
    "            'name': 'Token Limiting',\n",
    "            'strategy': 'Reduce max_tokens while maintaining quality',\n",
    "            'params': {'temperature': 0.7, 'top_p': 0.9, 'max_tokens': 200},\n",
    "            'system_message': \"Provide a concise but comprehensive response.\"\n",
    "        },\n",
    "        {\n",
    "            'name': 'Temperature Reduction',\n",
    "            'strategy': 'Lower temperature for more direct responses',\n",
    "            'params': {'temperature': 0.3, 'top_p': 0.9, 'max_tokens': 300},\n",
    "            'system_message': \"Provide a direct, focused response to the user's question.\"\n",
    "        },\n",
    "        {\n",
    "            'name': 'Structured Response',\n",
    "            'strategy': 'Guide LLM to produce structured, efficient responses',\n",
    "            'params': {'temperature': 0.4, 'top_p': 0.7, 'max_tokens': 250},\n",
    "            'system_message': \"Provide your response in exactly 3 bullet points, each with 1-2 sentences.\"\n",
    "        },\n",
    "        {\n",
    "            'name': 'Smart Prompting',\n",
    "            'strategy': 'More specific prompts to reduce unnecessary generation',\n",
    "            'params': {'temperature': 0.5, 'top_p': 0.8, 'max_tokens': 300},\n",
    "            'system_message': \"Answer in exactly 2 paragraphs with specific benefits and examples.\",\n",
    "            'modified_prompt': \"List exactly 5 key benefits of RAG systems for enterprises, with one sentence explanation each.\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    cost_results = []\n",
    "    \n",
    "    for strategy in optimization_strategies:\n",
    "        print(f\"\\nğŸ§ª **Testing: {strategy['name']}**\")\n",
    "        print(f\"   ğŸ’¡ Strategy: {strategy['strategy']}\")\n",
    "        print(f\"   âš™ï¸ Parameters: {strategy['params']}\")\n",
    "        \n",
    "        try:\n",
    "            # Use modified prompt if available, otherwise use base prompt\n",
    "            test_prompt = strategy.get('modified_prompt', base_prompt)\n",
    "            \n",
    "            result = explorer.generate_response(\n",
    "                prompt=test_prompt,\n",
    "                system_message=strategy['system_message'],\n",
    "                **strategy['params']\n",
    "            )\n",
    "            \n",
    "            metrics = result['metrics']\n",
    "            response = result['response']\n",
    "            \n",
    "            # Calculate cost efficiency metrics\n",
    "            cost_per_word = metrics['total_cost'] / metrics['word_count'] if metrics['word_count'] > 0 else 0\n",
    "            quality_indicators = {\n",
    "                'mentions_rag': 'rag' in response.lower(),\n",
    "                'mentions_enterprise': 'enterprise' in response.lower(),\n",
    "                'has_benefits': any(word in response.lower() for word in ['benefit', 'advantage', 'improve']),\n",
    "                'proper_length': 50 <= len(response.split()) <= 400,\n",
    "                'structured': any(marker in response for marker in ['1.', 'â€¢', '-', 'First', 'Second'])\n",
    "            }\n",
    "            \n",
    "            quality_score = sum(quality_indicators.values()) / len(quality_indicators)\n",
    "            cost_efficiency = quality_score / metrics['total_cost'] if metrics['total_cost'] > 0 else 0\n",
    "            \n",
    "            result_data = {\n",
    "                'strategy': strategy['name'],\n",
    "                'total_cost': metrics['total_cost'],\n",
    "                'word_count': metrics['word_count'],\n",
    "                'cost_per_word': cost_per_word,\n",
    "                'quality_score': quality_score,\n",
    "                'cost_efficiency': cost_efficiency,\n",
    "                'response_time': metrics['response_time'],\n",
    "                'token_usage': metrics['total_tokens']\n",
    "            }\n",
    "            \n",
    "            cost_results.append(result_data)\n",
    "            \n",
    "            print(f\"   ğŸ“Š Results:\")\n",
    "            print(f\"      â€¢ Total Cost: ${metrics['total_cost']:.4f}\")\n",
    "            print(f\"      â€¢ Words Generated: {metrics['word_count']}\")\n",
    "            print(f\"      â€¢ Cost per Word: ${cost_per_word:.6f}\")\n",
    "            print(f\"      â€¢ Quality Score: {quality_score:.2f}/1.0\")\n",
    "            print(f\"      â€¢ Cost Efficiency: {cost_efficiency:.2f}\")\n",
    "            print(f\"      â€¢ Response Preview: {response[:120]}...\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ Error: {e}\")\n",
    "    \n",
    "    return cost_results\n",
    "\n",
    "def analyze_cost_optimization(cost_results):\n",
    "    \"\"\"\n",
    "    Analyze cost optimization results and provide recommendations.\n",
    "    \"\"\"\n",
    "    print(\"\\nğŸ“ˆ Cost Optimization Analysis\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    if not cost_results:\n",
    "        print(\"âŒ No results to analyze\")\n",
    "        return\n",
    "    \n",
    "    # Sort by cost efficiency (quality/cost ratio)\n",
    "    sorted_results = sorted(cost_results, key=lambda x: x['cost_efficiency'], reverse=True)\n",
    "    \n",
    "    print(\"\\nğŸ† **Cost Efficiency Ranking** (Quality/Cost Ratio)\")\n",
    "    print(\"-\" * 55)\n",
    "    \n",
    "    for i, result in enumerate(sorted_results):\n",
    "        print(f\"{i+1}. **{result['strategy']}**\")\n",
    "        print(f\"   ğŸ’° Cost: ${result['total_cost']:.4f}\")\n",
    "        print(f\"   ğŸ“ Words: {result['word_count']}\")\n",
    "        print(f\"   ğŸ¯ Quality: {result['quality_score']:.2f}\")\n",
    "        print(f\"   âš¡ Efficiency: {result['cost_efficiency']:.2f}\")\n",
    "        print()\n",
    "    \n",
    "    # Calculate savings potential\n",
    "    baseline = next((r for r in cost_results if 'Baseline' in r['strategy']), None)\n",
    "    if baseline:\n",
    "        print(\"ğŸ’¡ **Cost Savings Analysis**\")\n",
    "        print(\"-\" * 28)\n",
    "        \n",
    "        for result in sorted_results[1:]:  # Skip baseline\n",
    "            cost_savings = baseline['total_cost'] - result['total_cost']\n",
    "            savings_percent = (cost_savings / baseline['total_cost']) * 100\n",
    "            quality_change = result['quality_score'] - baseline['quality_score']\n",
    "            \n",
    "            print(f\"**{result['strategy']}:**\")\n",
    "            print(f\"   ğŸ’° Cost Savings: ${cost_savings:.4f} ({savings_percent:.1f}%)\")\n",
    "            print(f\"   ğŸ“Š Quality Change: {quality_change:+.2f}\")\n",
    "            print(f\"   ğŸ¯ Recommended for: {'High-volume applications' if savings_percent > 30 else 'Quality-sensitive applications' if quality_change >= 0 else 'Cost-critical applications'}\")\n",
    "            print()\n",
    "    \n",
    "    return sorted_results\n",
    "\n",
    "def create_cost_optimization_framework():\n",
    "    \"\"\"\n",
    "    Create a practical framework for implementing cost optimization.\n",
    "    \"\"\"\n",
    "    print(\"ğŸ› ï¸ **Production Cost Optimization Framework**\")\n",
    "    print(\"=\" * 46)\n",
    "    \n",
    "    framework = {\n",
    "        'immediate_actions': [\n",
    "            \"Set appropriate max_tokens limits based on use case requirements\",\n",
    "            \"Use structured prompts to guide efficient response generation\", \n",
    "            \"Implement temperature reduction for straightforward queries\",\n",
    "            \"Add response format specifications to reduce unnecessary text\"\n",
    "        ],\n",
    "        'advanced_strategies': [\n",
    "            \"Implement query complexity analysis to adjust parameters dynamically\",\n",
    "            \"Use caching for frequently asked questions to avoid repeated API calls\",\n",
    "            \"Implement response streaming to improve perceived performance\",\n",
    "            \"Create parameter profiles for different user tiers (free vs premium)\"\n",
    "        ],\n",
    "        'monitoring_metrics': [\n",
    "            \"Cost per successful query\",\n",
    "            \"Average response quality score\",\n",
    "            \"Token utilization efficiency\",\n",
    "            \"User satisfaction vs cost trade-off ratio\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    for category, items in framework.items():\n",
    "        print(f\"\\n**{category.replace('_', ' ').title()}:**\")\n",
    "        for i, item in enumerate(items, 1):\n",
    "            print(f\"{i}. {item}\")\n",
    "    \n",
    "    # Create implementation guide\n",
    "    print(\"\\nğŸ“‹ **Implementation Checklist**\")\n",
    "    print(\"-\" * 25)\n",
    "    \n",
    "    checklist = [\n",
    "        \"â–¡ Analyze current parameter usage and costs\",\n",
    "        \"â–¡ Define quality thresholds for different use cases\", \n",
    "        \"â–¡ Implement A/B testing for parameter optimization\",\n",
    "        \"â–¡ Set up cost monitoring and alerting\",\n",
    "        \"â–¡ Create parameter profiles for different scenarios\",\n",
    "        \"â–¡ Implement automatic parameter adjustment based on load\",\n",
    "        \"â–¡ Monitor user satisfaction metrics alongside costs\",\n",
    "        \"â–¡ Regular review and optimization of parameter settings\"\n",
    "    ]\n",
    "    \n",
    "    for item in checklist:\n",
    "        print(item)\n",
    "    \n",
    "    return framework\n",
    "\n",
    "# Run cost optimization analysis\n",
    "cost_optimization_results = implement_cost_optimization_strategies()\n",
    "cost_analysis = analyze_cost_optimization(cost_optimization_results)\n",
    "cost_framework = create_cost_optimization_framework()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âš–ï¸ Production Trade-offs Key Insights:\n",
    "\n",
    "- **Speed-Optimized**: Minimal parameters for fastest response times\n",
    "- **Cost-Optimized**: Balanced token usage with acceptable quality  \n",
    "- **Quality-Optimized**: Higher parameters for comprehensive responses\n",
    "- **Balanced**: General-purpose settings for most production scenarios\n",
    "\n",
    "**Critical Production Considerations:**\n",
    "1. **Latency Requirements**: Sub-second vs. few-second response times\n",
    "2. **Cost Constraints**: High-volume vs. premium service pricing models\n",
    "3. **Quality Standards**: Acceptable accuracy vs. comprehensive responses\n",
    "4. **Scalability Needs**: Parameter impact on concurrent request handling\n",
    "\n",
    "<a id='5-2'></a>\n",
    "### 5.2 Cost Optimization Strategies\n",
    "\n",
    "**What we're doing:** Developing concrete strategies for minimizing LLM costs in production RAG systems while maintaining service quality standards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš–ï¸ Production Trade-offs Analysis\n",
      "===================================\n",
      "\n",
      "ğŸ§ª Testing 4 profiles across 4 scenarios...\n",
      "\n",
      "ğŸ“‹ **Scenario: High-Volume Customer Support**\n",
      "   ğŸ¯ Priority: speed_and_cost\n",
      "   ğŸ“ Requirements: Fast responses, low cost per query, acceptable accuracy\n",
      "\n",
      "   ğŸ”§ Testing speed_optimized profile...\n",
      "      ğŸ“Š Words: 74, Time: 2.97s, Cost: $0.0011\n",
      "      ğŸ“ˆ Scores - Quality: 0.91, Speed: 0.41, Cost: 0.00\n",
      "\n",
      "   ğŸ”§ Testing cost_optimized profile...\n",
      "      ğŸ“Š Words: 113, Time: 1.63s, Cost: $0.0016\n",
      "      ğŸ“ˆ Scores - Quality: 1.04, Speed: 0.67, Cost: 0.00\n",
      "\n",
      "   ğŸ”§ Testing quality_optimized profile...\n",
      "      ğŸ“Š Words: 146, Time: 3.48s, Cost: $0.0019\n",
      "      ğŸ“ˆ Scores - Quality: 1.15, Speed: 0.30, Cost: 0.00\n",
      "\n",
      "   ğŸ”§ Testing balanced profile...\n",
      "      ğŸ“Š Words: 159, Time: 2.55s, Cost: $0.0022\n",
      "      ğŸ“ˆ Scores - Quality: 1.20, Speed: 0.49, Cost: 0.00\n",
      "\n",
      "ğŸ“‹ **Scenario: Premium Advisory Service**\n",
      "   ğŸ¯ Priority: quality\n",
      "   ğŸ“ Requirements: Highest accuracy, detailed responses, cost secondary\n",
      "\n",
      "   ğŸ”§ Testing speed_optimized profile...\n",
      "      ğŸ“Š Words: 75, Time: 2.15s, Cost: $0.0011\n",
      "      ğŸ“ˆ Scores - Quality: 0.92, Speed: 0.57, Cost: 0.00\n",
      "\n",
      "   ğŸ”§ Testing cost_optimized profile...\n",
      "      ğŸ“Š Words: 106, Time: 6.25s, Cost: $0.0016\n",
      "      ğŸ“ˆ Scores - Quality: 1.02, Speed: 0.00, Cost: 0.00\n",
      "\n",
      "   ğŸ”§ Testing quality_optimized profile...\n",
      "      ğŸ“Š Words: 385, Time: 10.64s, Cost: $0.0051\n",
      "      ğŸ“ˆ Scores - Quality: 1.95, Speed: 0.00, Cost: 0.00\n",
      "\n",
      "   ğŸ”§ Testing balanced profile...\n",
      "      ğŸ“Š Words: 162, Time: 5.33s, Cost: $0.0026\n",
      "      ğŸ“ˆ Scores - Quality: 1.21, Speed: 0.00, Cost: 0.00\n",
      "\n",
      "ğŸ“‹ **Scenario: Educational Platform**\n",
      "   ğŸ¯ Priority: balance\n",
      "   ğŸ“ Requirements: Good quality explanations, reasonable cost, moderate speed\n",
      "\n",
      "   ğŸ”§ Testing speed_optimized profile...\n",
      "      ğŸ“Š Words: 85, Time: 2.25s, Cost: $0.0011\n",
      "      ğŸ“ˆ Scores - Quality: 0.95, Speed: 0.55, Cost: 0.00\n",
      "\n",
      "   ğŸ”§ Testing cost_optimized profile...\n",
      "      ğŸ“Š Words: 124, Time: 3.28s, Cost: $0.0016\n",
      "      ğŸ“ˆ Scores - Quality: 1.08, Speed: 0.34, Cost: 0.00\n",
      "\n",
      "   ğŸ”§ Testing quality_optimized profile...\n",
      "      ğŸ“Š Words: 247, Time: 5.12s, Cost: $0.0031\n",
      "      ğŸ“ˆ Scores - Quality: 1.49, Speed: 0.00, Cost: 0.00\n",
      "\n",
      "   ğŸ”§ Testing balanced profile...\n",
      "      ğŸ“Š Words: 207, Time: 3.99s, Cost: $0.0026\n",
      "      ğŸ“ˆ Scores - Quality: 1.36, Speed: 0.20, Cost: 0.00\n",
      "\n",
      "ğŸ“‹ **Scenario: Real-time Chat Assistant**\n",
      "   ğŸ¯ Priority: speed\n",
      "   ğŸ“ Requirements: Sub-second responses, consistent quality, cost-effective\n",
      "\n",
      "   ğŸ”§ Testing speed_optimized profile...\n",
      "      ğŸ“Š Words: 21, Time: 0.62s, Cost: $0.0003\n",
      "      ğŸ“ˆ Scores - Quality: 0.74, Speed: 0.88, Cost: 0.68\n",
      "\n",
      "   ğŸ”§ Testing cost_optimized profile...\n",
      "      ğŸ“Š Words: 21, Time: 1.34s, Cost: $0.0003\n",
      "      ğŸ“ˆ Scores - Quality: 0.74, Speed: 0.73, Cost: 0.68\n",
      "\n",
      "   ğŸ”§ Testing quality_optimized profile...\n",
      "      ğŸ“Š Words: 23, Time: 1.52s, Cost: $0.0004\n",
      "      ğŸ“ˆ Scores - Quality: 0.74, Speed: 0.70, Cost: 0.65\n",
      "\n",
      "   ğŸ”§ Testing balanced profile...\n",
      "      ğŸ“Š Words: 22, Time: 0.82s, Cost: $0.0003\n",
      "      ğŸ“ˆ Scores - Quality: 0.74, Speed: 0.84, Cost: 0.68\n",
      "\n",
      "ğŸ¯ Production RAG Parameter Recommendations\n",
      "================================================\n",
      "\n",
      "ğŸ“‹ **High-Volume Customer Support**\n",
      "   ğŸ† **Recommended Profile: cost_optimized**\n",
      "   ğŸ“Š Performance Metrics:\n",
      "      â€¢ Response Time: 1.63s\n",
      "      â€¢ Cost per Query: $0.0016\n",
      "      â€¢ Average Words: 113\n",
      "      â€¢ Quality Score: 1.04/1.0\n",
      "\n",
      "ğŸ“‹ **Premium Advisory Service**\n",
      "   ğŸ† **Recommended Profile: quality_optimized**\n",
      "   ğŸ“Š Performance Metrics:\n",
      "      â€¢ Response Time: 10.64s\n",
      "      â€¢ Cost per Query: $0.0051\n",
      "      â€¢ Average Words: 385\n",
      "      â€¢ Quality Score: 1.95/1.0\n",
      "\n",
      "ğŸ“‹ **Educational Platform**\n",
      "   ğŸ† **Recommended Profile: balanced**\n",
      "   ğŸ“Š Performance Metrics:\n",
      "      â€¢ Response Time: 3.99s\n",
      "      â€¢ Cost per Query: $0.0026\n",
      "      â€¢ Average Words: 207\n",
      "      â€¢ Quality Score: 1.36/1.0\n",
      "\n",
      "ğŸ“‹ **Real-time Chat Assistant**\n",
      "   ğŸ† **Recommended Profile: speed_optimized**\n",
      "   ğŸ“Š Performance Metrics:\n",
      "      â€¢ Response Time: 0.62s\n",
      "      â€¢ Cost per Query: $0.0003\n",
      "      â€¢ Average Words: 21\n",
      "      â€¢ Quality Score: 0.74/1.0\n",
      "\n",
      "ğŸš€ **Deployment Parameter Guide**\n",
      "===================================\n",
      "\n",
      "**High-Volume Customer Support:**\n",
      "```python\n",
      "llm_params = {'temperature': 0.1, 'top_p': 0.5, 'max_tokens': 100}\n",
      "# Rationale: Minimizes cost and latency for simple queries\n",
      "```\n",
      "\n",
      "**Premium Advisory Service:**\n",
      "```python\n",
      "llm_params = {'temperature': 0.5, 'top_p': 0.8, 'max_tokens': 500}\n",
      "# Rationale: Maximizes response quality and comprehensiveness\n",
      "```\n",
      "\n",
      "**Educational Platform:**\n",
      "```python\n",
      "llm_params = {'temperature': 0.4, 'top_p': 0.7, 'max_tokens': 250}\n",
      "# Rationale: Balances explanation quality with operational efficiency\n",
      "```\n",
      "\n",
      "**Real-time Chat Assistant:**\n",
      "```python\n",
      "llm_params = {'temperature': 0.1, 'top_p': 0.5, 'max_tokens': 100}\n",
      "# Rationale: Prioritizes speed for real-time interactions\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "def analyze_production_tradeoffs():\n",
    "    \"\"\"\n",
    "    Analyze the trade-offs between quality, speed, and cost for production RAG.\n",
    "    \"\"\"\n",
    "    print(\"âš–ï¸ Production Trade-offs Analysis\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    # Define production scenarios with different requirements\n",
    "    production_scenarios = [\n",
    "        {\n",
    "            'name': 'High-Volume Customer Support',\n",
    "            'priority': 'speed_and_cost',\n",
    "            'requirements': 'Fast responses, low cost per query, acceptable accuracy',\n",
    "            'test_prompt': 'How do I reset my password?'\n",
    "        },\n",
    "        {\n",
    "            'name': 'Premium Advisory Service', \n",
    "            'priority': 'quality',\n",
    "            'requirements': 'Highest accuracy, detailed responses, cost secondary',\n",
    "            'test_prompt': 'Provide a comprehensive analysis of market trends in AI.'\n",
    "        },\n",
    "        {\n",
    "            'name': 'Educational Platform',\n",
    "            'priority': 'balance',\n",
    "            'requirements': 'Good quality explanations, reasonable cost, moderate speed',\n",
    "            'test_prompt': 'Explain the concept of machine learning to a beginner.'\n",
    "        },\n",
    "        {\n",
    "            'name': 'Real-time Chat Assistant',\n",
    "            'priority': 'speed',\n",
    "            'requirements': 'Sub-second responses, consistent quality, cost-effective',\n",
    "            'test_prompt': 'What is the weather like today?'\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Define parameter profiles for different priorities\n",
    "    production_profiles = {\n",
    "        'speed_optimized': {\n",
    "            'params': {'temperature': 0.1, 'top_p': 0.5, 'max_tokens': 100},\n",
    "            'description': 'Minimal randomness, short responses, fastest generation'\n",
    "        },\n",
    "        'cost_optimized': {\n",
    "            'params': {'temperature': 0.3, 'top_p': 0.6, 'max_tokens': 150},\n",
    "            'description': 'Low token usage while maintaining basic quality'\n",
    "        },\n",
    "        'quality_optimized': {\n",
    "            'params': {'temperature': 0.5, 'top_p': 0.8, 'max_tokens': 500},\n",
    "            'description': 'Higher quality responses with more comprehensive content'\n",
    "        },\n",
    "        'balanced': {\n",
    "            'params': {'temperature': 0.4, 'top_p': 0.7, 'max_tokens': 250},\n",
    "            'description': 'Balanced approach for general production use'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    tradeoff_results = []\n",
    "    \n",
    "    print(f\"\\nğŸ§ª Testing {len(production_profiles)} profiles across {len(production_scenarios)} scenarios...\")\n",
    "    \n",
    "    for scenario in production_scenarios:\n",
    "        print(f\"\\nğŸ“‹ **Scenario: {scenario['name']}**\")\n",
    "        print(f\"   ğŸ¯ Priority: {scenario['priority']}\")\n",
    "        print(f\"   ğŸ“ Requirements: {scenario['requirements']}\")\n",
    "        \n",
    "        scenario_results = []\n",
    "        \n",
    "        for profile_name, profile in production_profiles.items():\n",
    "            print(f\"\\n   ğŸ”§ Testing {profile_name} profile...\")\n",
    "            \n",
    "            try:\n",
    "                result = explorer.generate_response(\n",
    "                    prompt=scenario['test_prompt'],\n",
    "                    system_message=\"You are a helpful assistant providing concise, accurate responses.\",\n",
    "                    **profile['params']\n",
    "                )\n",
    "                \n",
    "                # Calculate key metrics\n",
    "                metrics = result['metrics']\n",
    "                response = result['response']\n",
    "                \n",
    "                quality_score = (\n",
    "                    len(response.split()) / 100 +  # Word richness\n",
    "                    (1 if len(response) > 50 else 0.5) +  # Adequate length\n",
    "                    (1 if '.' in response else 0.5)  # Proper sentences\n",
    "                ) / 3\n",
    "                \n",
    "                speed_score = max(0, 1 - metrics['response_time'] / 5)  # Normalize to 0-1\n",
    "                cost_score = max(0, 1 - metrics['total_cost'] * 1000)  # Normalize to 0-1\n",
    "                \n",
    "                result_summary = {\n",
    "                    'scenario': scenario['name'],\n",
    "                    'profile': profile_name,\n",
    "                    'priority': scenario['priority'],\n",
    "                    'response_time': metrics['response_time'],\n",
    "                    'total_cost': metrics['total_cost'],\n",
    "                    'word_count': metrics['word_count'],\n",
    "                    'quality_score': quality_score,\n",
    "                    'speed_score': speed_score,\n",
    "                    'cost_score': cost_score,\n",
    "                    'response_preview': response[:100] + \"...\"\n",
    "                }\n",
    "                \n",
    "                scenario_results.append(result_summary)\n",
    "                tradeoff_results.append(result_summary)\n",
    "                \n",
    "                print(f\"      ğŸ“Š Words: {metrics['word_count']}, Time: {metrics['response_time']:.2f}s, Cost: ${metrics['total_cost']:.4f}\")\n",
    "                print(f\"      ğŸ“ˆ Scores - Quality: {quality_score:.2f}, Speed: {speed_score:.2f}, Cost: {cost_score:.2f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"      âŒ Error: {e}\")\n",
    "    \n",
    "    return tradeoff_results\n",
    "\n",
    "def create_production_recommendations(tradeoff_results):\n",
    "    \"\"\"\n",
    "    Create specific recommendations for production RAG deployments.\n",
    "    \"\"\"\n",
    "    print(\"\\nğŸ¯ Production RAG Parameter Recommendations\")\n",
    "    print(\"=\" * 48)\n",
    "    \n",
    "    # Group results by scenario\n",
    "    scenario_groups = {}\n",
    "    for result in tradeoff_results:\n",
    "        scenario = result['scenario']\n",
    "        if scenario not in scenario_groups:\n",
    "            scenario_groups[scenario] = []\n",
    "        scenario_groups[scenario].append(result)\n",
    "    \n",
    "    recommendations = {}\n",
    "    \n",
    "    for scenario, results in scenario_groups.items():\n",
    "        print(f\"\\nğŸ“‹ **{scenario}**\")\n",
    "        \n",
    "        # Find best profile for this scenario based on priority\n",
    "        priority = results[0]['priority']\n",
    "        \n",
    "        if priority == 'speed':\n",
    "            best_result = min(results, key=lambda x: x['response_time'])\n",
    "        elif priority == 'speed_and_cost':\n",
    "            best_result = min(results, key=lambda x: x['response_time'] + x['total_cost'] * 10)\n",
    "        elif priority == 'quality':\n",
    "            best_result = max(results, key=lambda x: x['quality_score'])\n",
    "        else:  # balanced\n",
    "            best_result = max(results, key=lambda x: (x['quality_score'] + x['speed_score'] + x['cost_score']) / 3)\n",
    "        \n",
    "        print(f\"   ğŸ† **Recommended Profile: {best_result['profile']}**\")\n",
    "        print(f\"   ğŸ“Š Performance Metrics:\")\n",
    "        print(f\"      â€¢ Response Time: {best_result['response_time']:.2f}s\")\n",
    "        print(f\"      â€¢ Cost per Query: ${best_result['total_cost']:.4f}\")\n",
    "        print(f\"      â€¢ Average Words: {best_result['word_count']}\")\n",
    "        print(f\"      â€¢ Quality Score: {best_result['quality_score']:.2f}/1.0\")\n",
    "        \n",
    "        recommendations[scenario] = {\n",
    "            'profile': best_result['profile'],\n",
    "            'metrics': {\n",
    "                'response_time': best_result['response_time'],\n",
    "                'cost': best_result['total_cost'],\n",
    "                'word_count': best_result['word_count'],\n",
    "                'quality_score': best_result['quality_score']\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    # Create deployment guide\n",
    "    print(\"\\nğŸš€ **Deployment Parameter Guide**\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    deployment_guide = {\n",
    "        'High-Volume Customer Support': {\n",
    "            'recommended_params': {'temperature': 0.1, 'top_p': 0.5, 'max_tokens': 100},\n",
    "            'rationale': 'Minimizes cost and latency for simple queries'\n",
    "        },\n",
    "        'Premium Advisory Service': {\n",
    "            'recommended_params': {'temperature': 0.5, 'top_p': 0.8, 'max_tokens': 500},\n",
    "            'rationale': 'Maximizes response quality and comprehensiveness'\n",
    "        },\n",
    "        'Educational Platform': {\n",
    "            'recommended_params': {'temperature': 0.4, 'top_p': 0.7, 'max_tokens': 250},\n",
    "            'rationale': 'Balances explanation quality with operational efficiency'\n",
    "        },\n",
    "        'Real-time Chat Assistant': {\n",
    "            'recommended_params': {'temperature': 0.1, 'top_p': 0.5, 'max_tokens': 100},\n",
    "            'rationale': 'Prioritizes speed for real-time interactions'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for use_case, guide in deployment_guide.items():\n",
    "        print(f\"\\n**{use_case}:**\")\n",
    "        print(f\"```python\")\n",
    "        print(f\"llm_params = {guide['recommended_params']}\")\n",
    "        print(f\"# Rationale: {guide['rationale']}\")\n",
    "        print(f\"```\")\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# Run production trade-offs analysis\n",
    "production_tradeoffs = analyze_production_tradeoffs()\n",
    "production_recommendations = create_production_recommendations(production_tradeoffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ’° Cost Optimization Key Insights:\n",
    "\n",
    "- **Token Limiting**: Most effective immediate cost reduction strategy\n",
    "- **Structured Responses**: Balance cost savings with quality maintenance\n",
    "- **Temperature Reduction**: Minimal cost impact but improves consistency\n",
    "- **Smart Prompting**: Highest quality-to-cost ratio when implemented well\n",
    "\n",
    "**Production Cost Optimization Strategy:**\n",
    "1. **Immediate (0-30 days)**: Implement token limits and structured prompts\n",
    "2. **Short-term (1-3 months)**: Deploy parameter profiles for different use cases\n",
    "3. **Long-term (3-6 months)**: Implement dynamic parameter adjustment based on load and complexity\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ‰ Summary: LLM Parameter Mastery for Production RAG\n",
    "\n",
    "## ğŸ† What You've Accomplished\n",
    "\n",
    "You've mastered the **science and art of LLM parameter optimization** for production RAG systems, gaining deep insights into how parameters affect quality, cost, and performance.\n",
    "\n",
    "### ğŸ§  **Core Concepts Mastered:**\n",
    "\n",
    "1. **Temperature Control**\n",
    "   - Systematic understanding of randomness vs. consistency trade-offs\n",
    "   - Optimal temperature ranges for different RAG use cases\n",
    "   - Impact on response quality and computational cost\n",
    "\n",
    "2. **Top-p (Nucleus Sampling)**\n",
    "   - Diversity control through probability thresholds\n",
    "   - Interaction effects with temperature settings\n",
    "   - Applications for various content generation needs\n",
    "\n",
    "3. **Parameter Combinations**\n",
    "   - Scientific approach to testing parameter interactions\n",
    "   - Optimization matrices for systematic evaluation\n",
    "   - Use case-specific parameter profile creation\n",
    "\n",
    "4. **Production Optimization**\n",
    "   - Quality vs. speed vs. cost trade-off analysis\n",
    "   - Cost reduction strategies with measurable impact\n",
    "   - Scalability considerations for high-volume deployments\n",
    "\n",
    "### ğŸš€ **Production-Ready Knowledge:**\n",
    "\n",
    "- **Cost Optimization**: Strategies to reduce API costs by 30-60% while maintaining quality\n",
    "- **Performance Tuning**: Parameter settings optimized for different latency requirements\n",
    "- **Use Case Adaptation**: Specific recommendations for customer support, education, advisory services\n",
    "- **Monitoring Frameworks**: Metrics and tools for ongoing parameter optimization\n",
    "\n",
    "### ğŸ’¡ **Key Strategic Insights:**\n",
    "\n",
    "1. **Parameter Context Matters**: Optimal settings depend heavily on use case, user expectations, and business constraints\n",
    "2. **Cost-Quality Balance**: Small parameter adjustments can yield significant cost savings with minimal quality impact\n",
    "3. **Systematic Approach**: Data-driven parameter tuning outperforms intuitive guessing\n",
    "4. **Production Considerations**: Real-world factors like latency and scalability must drive parameter decisions\n",
    "\n",
    "### ğŸ¯ **Production Parameter Recommendations:**\n",
    "\n",
    "| **Use Case** | **Temperature** | **Top-p** | **Max Tokens** | **Priority** |\n",
    "|-------------|----------------|-----------|----------------|--------------|\n",
    "| **Customer Support** | 0.1-0.3 | 0.5-0.7 | 100-200 | Speed + Cost |\n",
    "| **Educational Content** | 0.4-0.7 | 0.7-0.8 | 250-400 | Balance |\n",
    "| **Premium Advisory** | 0.5-0.8 | 0.8-0.9 | 400-600 | Quality |\n",
    "| **Real-time Chat** | 0.1-0.2 | 0.5-0.6 | 50-150 | Speed |\n",
    "| **Content Creation** | 0.7-0.9 | 0.8-0.9 | 300-500 | Creativity |\n",
    "\n",
    "### ğŸ”® **Advanced Applications:**\n",
    "\n",
    "1. **Dynamic Parameter Adjustment**: Adapt parameters based on query complexity and user context\n",
    "2. **A/B Testing Frameworks**: Continuous optimization through systematic testing\n",
    "3. **Multi-Model Strategies**: Parameter optimization across different LLM providers\n",
    "4. **Cost Prediction Models**: Forecasting and budgeting for parameter-driven cost variations\n",
    "\n",
    "### âš¡ **Production Deployment Checklist:**\n",
    "\n",
    "- [ ] Parameter profiles created for all use cases\n",
    "- [ ] Cost monitoring and alerting implemented\n",
    "- [ ] A/B testing framework for ongoing optimization\n",
    "- [ ] Quality thresholds defined and measured\n",
    "- [ ] Scalability testing with production-level loads\n",
    "- [ ] Fallback strategies for parameter optimization failures\n",
    "- [ ] Documentation for parameter decision rationale\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ“ Congratulations!** You now possess **expert-level knowledge** of LLM parameter optimization for production RAG systems. You understand not just the technical mechanics, but the strategic decision-making process that separates successful RAG deployments from failed ones.\n",
    "\n",
    "**Your parameter optimization skills** will directly impact the success, cost-effectiveness, and user satisfaction of every RAG system you build or optimize!\n",
    "\n",
    "*Ready to deploy cost-effective, high-quality RAG systems with optimized LLM parameters? Your systematic approach to parameter tuning will give you a significant competitive advantage!* âœ¨"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
