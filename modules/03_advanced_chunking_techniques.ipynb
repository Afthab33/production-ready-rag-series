{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# RAG Series - Module 3: Advanced Chunking Techniques\n",
    "\n",
    "Welcome to Module 3! Building on the foundation from Module 2, we now explore **cutting-edge chunking techniques** that leverage AI, semantics, and intelligent analysis to create optimal chunks for RAG systems.\n",
    "\n",
    "## Table of Contents\n",
    "- [1 - Introduction](#1)\n",
    "  - [1.1 Setup and Installation](#1-1)\n",
    "  - [1.2 Advanced Data Preparation](#1-2)\n",
    "- [2 - Semantic Chunking](#2)\n",
    "  - [2.1 LangChain Semantic Chunker](#2-1)\n",
    "- [3 - Agentic Chunking](#3)\n",
    "  - [3.1 LLM-Powered Chunk Analysis](#3-1)\n",
    "- [4 - Proposition-Based Chunking](#4)\n",
    "  - [4.1 Atomic Fact Extraction](#4-1)\n",
    "- [5 - Production Evaluation](#6)\n",
    "  - [5.1 Comparative Analysis](#6-1)\n",
    "\n",
    "---\n",
    "\n",
    "## üß† What Makes Advanced Chunking Different?\n",
    "\n",
    "While traditional chunking relies on **static rules** (character counts, separators), advanced techniques use:\n",
    "\n",
    "- **üéØ Semantic Understanding**: Chunks based on meaning, not just structure\n",
    "- **ü§ñ AI-Powered Analysis**: LLMs determine optimal chunk boundaries\n",
    "- **üîó Context Preservation**: Intelligent overlap and relationship modeling\n",
    "- **üìä Multi-Vector Representations**: Different embeddings for summaries vs. details\n",
    "- **‚ö° Dynamic Adaptation**: Chunking strategies that adapt to content type\n",
    "\n",
    "These techniques can **dramatically improve RAG performance** but come with trade-offs in complexity and computational cost.\n",
    "\n",
    "---\n",
    "\n",
    "**Technologies we'll explore:**\n",
    "- **LangChain Experimental**: Semantic chunkers and advanced splitters\n",
    "- **OpenAI GPT Models**: For intelligent content analysis\n",
    "- **Custom Algorithms**: Proposition extraction and clustering\n",
    "- **Multi-Vector Storage**: Late interaction patterns with Pinecone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "## 1 - Introduction\n",
    "\n",
    "---\n",
    "\n",
    "<a id='1-1'></a>\n",
    "### 1.1 Setup and Installation\n",
    "\n",
    "For advanced chunking techniques, we need additional packages beyond the standard LangChain suite.\n",
    "\n",
    "**What we're doing:** Installing cutting-edge packages for semantic analysis, experimental LangChain features, and advanced text processing capabilities required for intelligent chunking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (0.3.26)\n",
      "Requirement already satisfied: langchain-pinecone in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (0.2.8)\n",
      "Requirement already satisfied: langchain-openai in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (0.3.27)\n",
      "Collecting langchain-experimental\n",
      "  Downloading langchain_experimental-0.3.4-py3-none-any.whl (209 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from langchain) (2.0.41)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from langchain) (2.32.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from langchain) (2.11.6)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from langchain) (0.3.45)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from langchain) (0.3.66)\n",
      "Requirement already satisfied: pinecone[asyncio]<8.0.0,>=6.0.0 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from langchain-pinecone) (7.2.0)\n",
      "Requirement already satisfied: numpy>=1.26.4 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from langchain-pinecone) (2.2.6)\n",
      "Requirement already satisfied: langchain-tests<1.0.0,>=0.3.7 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from langchain-pinecone) (0.3.20)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.86.0 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from langchain-openai) (1.95.1)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from langchain-openai) (0.9.0)\n",
      "Requirement already satisfied: langchain-community<0.4.0,>=0.3.0 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from langchain-experimental) (0.3.26)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.12.12)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.6.7)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (9.1.2)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.10.0)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.4.0)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (24.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (4.13.2)\n",
      "Requirement already satisfied: pytest-benchmark in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (5.1.0)\n",
      "Requirement already satisfied: pytest-asyncio<1,>=0.20 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (0.26.0)\n",
      "Requirement already satisfied: pytest<9,>=7 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (8.4.1)\n",
      "Requirement already satisfied: pytest-socket<1,>=0.6.0 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (0.7.0)\n",
      "Requirement already satisfied: pytest-recording in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (0.13.4)\n",
      "Requirement already satisfied: pytest-codspeed in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (3.2.0)\n",
      "Requirement already satisfied: vcrpy>=7.0 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (7.0.0)\n",
      "Requirement already satisfied: syrupy<5,>=4 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (4.9.1)\n",
      "Requirement already satisfied: httpx<1,>=0.25.0 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (0.28.1)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from langsmith>=0.1.17->langchain) (3.10.18)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (4.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (0.10.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: tqdm>4 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (4.67.1)\n",
      "Requirement already satisfied: sniffio in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (2025.4.26)\n",
      "Requirement already satisfied: pinecone-plugin-assistant<2.0.0,>=1.6.0 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (1.7.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (2.9.0.post0)\n",
      "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (0.0.7)\n",
      "Requirement already satisfied: aiohttp-retry<3.0.0,>=2.9.1 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (2.9.1)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.4.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.3.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.3.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.6.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (25.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.20.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (6.4.4)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.86.0->langchain-openai) (1.3.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from httpx<1,>=0.25.0->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.25.0->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (3.0.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.1.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from pytest<9,>=7->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (1.6.0)\n",
      "Requirement already satisfied: iniconfig>=1 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from pytest<9,>=7->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (2.1.0)\n",
      "Requirement already satisfied: pygments>=2.7.2 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from pytest<9,>=7->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (2.19.1)\n",
      "Requirement already satisfied: tomli>=1 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from pytest<9,>=7->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (2.2.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from python-dateutil>=2.5.3->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (1.17.0)\n",
      "Requirement already satisfied: wrapt in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from vcrpy>=7.0->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (1.17.2)\n",
      "Requirement already satisfied: py-cpuinfo in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from pytest-benchmark->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (9.0.0)\n",
      "Requirement already satisfied: rich>=13.8.1 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from pytest-codspeed->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (14.0.0)\n",
      "Requirement already satisfied: cffi>=1.17.1 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from pytest-codspeed->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from cffi>=1.17.1->pytest-codspeed->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (2.22)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from rich>=13.8.1->pytest-codspeed->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (3.0.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.1.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=13.8.1->pytest-codspeed->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (0.1.2)\n",
      "Installing collected packages: langchain-experimental\n",
      "Successfully installed langchain-experimental-0.3.4\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pinecone in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (7.2.0)\n",
      "Requirement already satisfied: tiktoken in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (0.9.0)\n",
      "Requirement already satisfied: requests in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (2.32.4)\n",
      "Requirement already satisfied: tqdm in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (4.67.1)\n",
      "Collecting uuid\n",
      "  Downloading uuid-1.30.tar.gz (5.8 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (2.2.6)\n",
      "Requirement already satisfied: scipy in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (1.15.3)\n",
      "Requirement already satisfied: scikit-learn in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (1.7.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from pinecone) (4.13.2)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from pinecone) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from pinecone) (2025.4.26)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from pinecone) (2.9.0.post0)\n",
      "Requirement already satisfied: pinecone-plugin-assistant<2.0.0,>=1.6.0 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from pinecone) (1.7.0)\n",
      "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from pinecone) (0.0.7)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from requests) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: packaging<25.0,>=24.2 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (24.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from python-dateutil>=2.5.3->pinecone) (1.17.0)\n",
      "Installing collected packages: uuid\n",
      "\u001b[33m  DEPRECATION: uuid is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\u001b[0m\u001b[33m\n",
      "\u001b[0m  Running setup.py install for uuid ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed uuid-1.30\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-5.1.0-py3-none-any.whl (483 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m483.4/483.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: transformers in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (4.52.4)\n",
      "Collecting spacy\n",
      "  Downloading spacy-3.8.7-cp310-cp310-macosx_11_0_arm64.whl (6.3 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: nltk in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (3.9.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from sentence-transformers) (4.13.2)\n",
      "Requirement already satisfied: scipy in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from sentence-transformers) (1.15.3)\n",
      "Requirement already satisfied: tqdm in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: scikit-learn in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from sentence-transformers) (1.7.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from sentence-transformers) (0.33.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from sentence-transformers) (2.8.0)\n",
      "Requirement already satisfied: Pillow in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from sentence-transformers) (11.2.1)\n",
      "Requirement already satisfied: requests in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: filelock in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from transformers) (0.21.1)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Using cached spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Using cached catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from spacy) (2.11.6)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.10-cp310-cp310-macosx_11_0_arm64.whl (127 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.3/127.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting weasel<0.5.0,>=0.1.0\n",
      "  Using cached weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from spacy) (0.16.0)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Using cached cymem-2.0.11-cp310-cp310-macosx_11_0_arm64.whl (41 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1\n",
      "  Using cached wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.13-cp310-cp310-macosx_11_0_arm64.whl (26 kB)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Using cached langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Using cached srsly-2.5.1-cp310-cp310-macosx_11_0_arm64.whl (634 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4\n",
      "  Using cached thinc-8.3.6-cp310-cp310-macosx_11_0_arm64.whl (844 kB)\n",
      "Requirement already satisfied: jinja2 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from spacy) (65.5.0)\n",
      "Requirement already satisfied: joblib in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: click in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.5.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.3)\n",
      "Collecting language-data>=1.2\n",
      "  Using cached language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from requests->transformers) (2025.4.26)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from requests->transformers) (3.4.2)\n",
      "Collecting blis<1.4.0,>=1.3.0\n",
      "  Using cached blis-1.3.0-cp310-cp310-macosx_11_0_arm64.whl (1.3 MB)\n",
      "Collecting confection<1.0.0,>=0.0.1\n",
      "  Using cached confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Requirement already satisfied: networkx in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: rich>=10.11.0 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy) (14.0.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0\n",
      "  Downloading cloudpathlib-0.21.1-py3-none-any.whl (52 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting smart-open<8.0.0,>=5.2.1\n",
      "  Downloading smart_open-7.3.0.post1-py3-none-any.whl (61 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m61.9/61.9 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from jinja2->spacy) (3.0.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Collecting marisa-trie>=1.1.0\n",
      "  Downloading marisa_trie-1.3.0-cp310-cp310-macosx_11_0_arm64.whl (157 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m157.1/157.1 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: wrapt in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Installing collected packages: cymem, wasabi, spacy-loggers, spacy-legacy, smart-open, murmurhash, marisa-trie, cloudpathlib, catalogue, blis, srsly, preshed, language-data, langcodes, confection, weasel, thinc, spacy, sentence-transformers\n",
      "Successfully installed blis-1.3.0 catalogue-2.0.10 cloudpathlib-0.21.1 confection-0.1.5 cymem-2.0.11 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.3.0 murmurhash-1.0.13 preshed-3.0.10 sentence-transformers-5.1.0 smart-open-7.3.0.post1 spacy-3.8.7 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.6 wasabi-1.1.3 weasel-0.4.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages for advanced chunking\n",
    "%pip install langchain langchain-pinecone langchain-openai langchain-experimental\n",
    "%pip install pinecone tiktoken requests tqdm uuid numpy scipy scikit-learn\n",
    "%pip install sentence-transformers transformers spacy nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "Now let's import our libraries and set up the advanced chunking environment:\n",
    "\n",
    "**What we're doing:** Importing all necessary libraries including experimental features, setting up API keys, and initializing models for semantic analysis and LLM-powered chunking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Semantic Chunker available\n",
      "‚úÖ Advanced chunking environment configured!\n",
      "‚úÖ NLTK data downloaded\n"
     ]
    }
   ],
   "source": [
    "# Core imports\n",
    "from typing import List, Dict, Tuple, Optional, Any\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.cluster import DBSCAN\n",
    "import nltk\n",
    "from uuid import uuid4\n",
    "import time\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Advanced chunking imports\n",
    "try:\n",
    "    from langchain_experimental.text_splitter import SemanticChunker\n",
    "    print(\"‚úÖ Semantic Chunker available\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è Semantic Chunker not available - will use custom implementation\")\n",
    "    SemanticChunker = None\n",
    "\n",
    "# Pinecone\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "# Set your API keys here\n",
    "OPENAI_API_KEY = \"your-openai-api-key-here\"      # Get from https://platform.openai.com/account/api-keys\n",
    "PINECONE_API_KEY = \"your-pinecone-api-key-here\"  # Get from https://app.pinecone.io\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n",
    "\n",
    "print(\"‚úÖ Advanced chunking environment configured!\")\n",
    "\n",
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    print(\"‚úÖ NLTK data downloaded\")\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è NLTK download failed - some features may be limited\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "**Result:** ‚úÖ Advanced environment setup complete! We now have access to experimental chunking features, semantic analysis tools, and LLM-powered content understanding capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "<a id='1-2'></a>\n",
    "### 1.2 Advanced Data Preparation\n",
    "\n",
    "For advanced chunking, we need diverse, complex content that showcases different techniques. Let's gather multiple document types.\n",
    "\n",
    "**What we're doing:** Loading multiple document types (technical documentation, academic papers, code files) to demonstrate how advanced chunking techniques handle different content structures and semantic patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Loading Git documentation...\n",
      "üî¨ Adding research content...\n",
      "üíª Adding mixed technical content...\n",
      "‚úÖ Loaded 5 diverse documents for advanced chunking\n",
      "üìä Dataset statistics:\n",
      "   Total characters: 31,159\n",
      "   Total words: 4,821\n",
      "   Document types: {'technical_tutorial', 'academic_research', 'technical_documentation'}\n"
     ]
    }
   ],
   "source": [
    "# Load diverse content for advanced chunking demonstrations\n",
    "def load_diverse_content():\n",
    "    \"\"\"\n",
    "    Load multiple types of documents for advanced chunking testing.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    \n",
    "    # 1. Technical documentation (Git book)\n",
    "    print(\"üìö Loading Git documentation...\")\n",
    "    git_urls = [\n",
    "        \"https://raw.githubusercontent.com/progit/progit2/main/book/01-introduction/sections/what-is-git.asc\",\n",
    "        \"https://raw.githubusercontent.com/progit/progit2/main/book/02-git-basics/sections/getting-a-repository.asc\",\n",
    "        \"https://raw.githubusercontent.com/progit/progit2/main/book/03-git-branching/sections/basic-branching-and-merging.asc\"\n",
    "    ]\n",
    "    \n",
    "    for i, url in enumerate(git_urls):\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            doc = Document(\n",
    "                page_content=response.text,\n",
    "                metadata={\n",
    "                    'source': f'git_book_chapter_{i+1}',\n",
    "                    'type': 'technical_documentation',\n",
    "                    'complexity': 'intermediate',\n",
    "                    'url': url\n",
    "                }\n",
    "            )\n",
    "            documents.append(doc)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Failed to load {url}: {e}\")\n",
    "    \n",
    "    # 2. Academic/research content\n",
    "    print(\"üî¨ Adding research content...\")\n",
    "    academic_content = \"\"\"\n",
    "    # Retrieval-Augmented Generation: A Comprehensive Survey\n",
    "    \n",
    "    ## Abstract\n",
    "    Retrieval-Augmented Generation (RAG) represents a paradigm shift in natural language processing, combining the parametric knowledge of large language models with non-parametric retrieval from external knowledge bases. This approach addresses fundamental limitations of standalone generative models, including knowledge cutoffs, hallucination tendencies, and inability to access real-time information.\n",
    "    \n",
    "    ## Introduction\n",
    "    The integration of retrieval mechanisms with generative models has emerged as a critical advancement in AI systems. Traditional language models, while powerful, suffer from several key limitations. First, their knowledge is frozen at training time, making them unable to access new information. Second, they are prone to generating plausible-sounding but factually incorrect information, a phenomenon known as hallucination.\n",
    "    \n",
    "    ## Methodology\n",
    "    RAG systems typically follow a two-stage process: retrieval and generation. The retrieval stage involves searching through external knowledge sources to find relevant information given a query. This search can be performed using various techniques, including dense vector search, sparse keyword matching, or hybrid approaches that combine both methodologies.\n",
    "    \n",
    "    The generation stage takes the retrieved context and the original query as input to a language model, which then generates a response that incorporates the retrieved information. This approach allows the model to produce answers that are both contextually relevant and factually grounded.\n",
    "    \n",
    "    ## Evaluation Metrics\n",
    "    Evaluating RAG systems requires consideration of multiple dimensions. Retrieval quality can be measured using traditional information retrieval metrics such as precision, recall, and normalized discounted cumulative gain (NDCG). Generation quality involves assessing factual accuracy, relevance, coherence, and completeness of responses.\n",
    "    \n",
    "    Recent research has introduced specialized metrics for RAG evaluation, including faithfulness (how well the generated text aligns with retrieved sources), answer relevance (how well the answer addresses the query), and context precision (quality of retrieved context).\n",
    "    \n",
    "    ## Challenges and Future Directions\n",
    "    Despite significant progress, RAG systems face several challenges. Information quality control remains difficult, as retrieved documents may contain outdated, biased, or incorrect information. Computational efficiency is another concern, as the retrieval process adds latency to response generation.\n",
    "    \n",
    "    Future research directions include improving retrieval algorithms, developing better evaluation frameworks, and exploring novel architectures that more tightly integrate retrieval and generation components.\n",
    "    \"\"\"\n",
    "    \n",
    "    doc = Document(\n",
    "        page_content=academic_content,\n",
    "        metadata={\n",
    "            'source': 'rag_survey_paper',\n",
    "            'type': 'academic_research',\n",
    "            'complexity': 'advanced',\n",
    "            'domain': 'machine_learning'\n",
    "        }\n",
    "    )\n",
    "    documents.append(doc)\n",
    "    \n",
    "    # 3. Mixed content (instructions + code + theory)\n",
    "    print(\"üíª Adding mixed technical content...\")\n",
    "    mixed_content = \"\"\"\n",
    "    # Advanced Vector Search Implementation Guide\n",
    "    \n",
    "    Vector search has revolutionized information retrieval by enabling semantic similarity matching rather than exact keyword matching. This guide covers advanced implementation techniques.\n",
    "    \n",
    "    ## Core Concepts\n",
    "    \n",
    "    ### Dense Vector Representations\n",
    "    Dense vectors encode semantic meaning in high-dimensional space. Unlike sparse representations (like TF-IDF), dense vectors capture contextual relationships between concepts.\n",
    "    \n",
    "    ```python\n",
    "    import numpy as np\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    \n",
    "    # Initialize embedding model\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    \n",
    "    # Generate embeddings\n",
    "    texts = [\"Machine learning fundamentals\", \"Deep learning architectures\"]\n",
    "    embeddings = model.encode(texts)\n",
    "    \n",
    "    # Calculate similarity\n",
    "    similarity = np.dot(embeddings[0], embeddings[1]) / (np.linalg.norm(embeddings[0]) * np.linalg.norm(embeddings[1]))\n",
    "    ```\n",
    "    \n",
    "    ### Similarity Metrics\n",
    "    \n",
    "    Different similarity metrics serve different purposes:\n",
    "    \n",
    "    1. **Cosine Similarity**: Measures angle between vectors, good for text\n",
    "    2. **Euclidean Distance**: Measures geometric distance, sensitive to magnitude\n",
    "    3. **Dot Product**: Fast computation, unnormalized similarity\n",
    "    \n",
    "    ## Implementation Strategies\n",
    "    \n",
    "    ### Hybrid Search Architecture\n",
    "    Modern search systems combine multiple retrieval methods:\n",
    "    \n",
    "    ```python\n",
    "    class HybridRetriever:\n",
    "        def __init__(self, dense_index, sparse_index, alpha=0.7):\n",
    "            self.dense_index = dense_index\n",
    "            self.sparse_index = sparse_index\n",
    "            self.alpha = alpha  # Weight for dense vs sparse\n",
    "        \n",
    "        def search(self, query, top_k=10):\n",
    "            # Dense search\n",
    "            dense_results = self.dense_index.search(query, top_k)\n",
    "            \n",
    "            # Sparse search  \n",
    "            sparse_results = self.sparse_index.search(query, top_k)\n",
    "            \n",
    "            # Combine results\n",
    "            return self._fusion(dense_results, sparse_results)\n",
    "    ```\n",
    "    \n",
    "    ### Performance Optimization\n",
    "    \n",
    "    Vector search performance depends on several factors:\n",
    "    \n",
    "    - **Index Structure**: HNSW, IVF, or LSH for approximate search\n",
    "    - **Dimensionality**: Higher dimensions capture more information but increase compute\n",
    "    - **Quantization**: Reduce memory usage with minimal quality loss\n",
    "    - **Caching**: Store frequently accessed embeddings in memory\n",
    "    \n",
    "    ## Advanced Techniques\n",
    "    \n",
    "    ### Multi-Vector Approaches\n",
    "    \n",
    "    Instead of single embeddings per document, use multiple representations:\n",
    "    \n",
    "    1. **Summary Embeddings**: High-level document overview\n",
    "    2. **Chunk Embeddings**: Detailed section representations\n",
    "    3. **Keyword Embeddings**: Important term vectors\n",
    "    \n",
    "    ### Query Enhancement\n",
    "    \n",
    "    Improve retrieval quality through query processing:\n",
    "    \n",
    "    - **Query Expansion**: Add related terms using embeddings\n",
    "    - **Query Rewriting**: Rephrase for better matching\n",
    "    - **Multi-Query**: Generate multiple query variants\n",
    "    \n",
    "    ## Production Considerations\n",
    "    \n",
    "    Deploying vector search in production requires careful attention to:\n",
    "    \n",
    "    - **Latency Requirements**: Sub-100ms response times\n",
    "    - **Scalability**: Handle millions of vectors efficiently\n",
    "    - **Cost Optimization**: Balance performance vs infrastructure costs\n",
    "    - **Monitoring**: Track search quality and system performance\n",
    "    \n",
    "    The key to successful vector search implementation lies in understanding your specific use case and optimizing accordingly.\n",
    "    \"\"\"\n",
    "    \n",
    "    doc = Document(\n",
    "        page_content=mixed_content,\n",
    "        metadata={\n",
    "            'source': 'vector_search_guide',\n",
    "            'type': 'technical_tutorial',\n",
    "            'complexity': 'advanced',\n",
    "            'contains_code': True,\n",
    "            'domain': 'information_retrieval'\n",
    "        }\n",
    "    )\n",
    "    documents.append(doc)\n",
    "    \n",
    "    print(f\"‚úÖ Loaded {len(documents)} diverse documents for advanced chunking\")\n",
    "    \n",
    "    # Display document statistics\n",
    "    total_chars = sum(len(doc.page_content) for doc in documents)\n",
    "    total_words = sum(len(doc.page_content.split()) for doc in documents)\n",
    "    \n",
    "    print(f\"üìä Dataset statistics:\")\n",
    "    print(f\"   Total characters: {total_chars:,}\")\n",
    "    print(f\"   Total words: {total_words:,}\")\n",
    "    print(f\"   Document types: {set(doc.metadata['type'] for doc in documents)}\")\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# Load our diverse content\n",
    "documents = load_diverse_content()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "**Result:** ‚úÖ Successfully loaded diverse content including technical documentation, academic research, and mixed tutorial content. This variety will showcase how different advanced chunking techniques handle various content structures and semantic patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "## 2 - Semantic Chunking\n",
    "\n",
    "---\n",
    "\n",
    "**Semantic chunking** goes beyond structural markers to understand content meaning. Instead of splitting on fixed boundaries, it analyzes semantic similarity between sentences and paragraphs to create meaningful chunks.\n",
    "\n",
    "### üéØ How Semantic Chunking Works:\n",
    "\n",
    "1. **Sentence Embeddings**: Convert each sentence to vector representation\n",
    "2. **Similarity Analysis**: Compare adjacent sentence embeddings\n",
    "3. **Boundary Detection**: Identify semantic breaks where similarity drops\n",
    "4. **Chunk Formation**: Group sentences with high semantic coherence\n",
    "\n",
    "### ‚úÖ Benefits:\n",
    "- Preserves topical coherence\n",
    "- Creates natural information boundaries\n",
    "- Improves retrieval relevance\n",
    "- Adapts to content structure\n",
    "\n",
    "### ‚ùå Trade-offs:\n",
    "- Higher computational cost\n",
    "- Embedding model dependency\n",
    "- Less predictable chunk sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "<a id='2-1'></a>\n",
    "### 2.1 LangChain Semantic Chunker\n",
    "\n",
    "LangChain's experimental `SemanticChunker` uses embeddings to identify semantic boundaries.\n",
    "\n",
    "**What we're doing:** Implementing LangChain's SemanticChunker to automatically detect topic boundaries using embedding similarity analysis. This creates chunks based on semantic coherence rather than arbitrary size limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LangChain SemanticChunker initialized\n",
      "üß† Testing LangChain Semantic Chunking...\n",
      "\n",
      "üìÑ Processing document 1: git_book_chapter_1\n",
      "   üìä Created 4 semantic chunks\n",
      "   üìè Average chunk size: 350.8 words\n",
      "   üìù Sample chunk: [[what_is_git_section]]\n",
      "=== What is Git? So, what is Git in a nutshell? This is an important section to absorb, because if you understand what Git is and the fundamentals of how it works, then using G...\n",
      "\n",
      "üìÑ Processing document 2: git_book_chapter_2\n",
      "   üìä Created 2 semantic chunks\n",
      "   üìè Average chunk size: 327.5 words\n",
      "   üìù Sample chunk: [[_getting_a_repo]]\n",
      "=== Getting a Git Repository\n",
      "\n",
      "You typically obtain a Git repository in one of two ways:\n",
      "\n",
      "1. You can take a local directory that is currently not under version control, and turn it ...\n",
      "\n",
      "üìÑ Processing document 3: git_book_chapter_3\n",
      "   üìä Created 5 semantic chunks\n",
      "   üìè Average chunk size: 410.0 words\n",
      "   üìù Sample chunk: === Basic Branching and Merging\n",
      "\n",
      "Let's go through a simple example of branching and merging with a workflow that you might use in the real world. You'll follow these steps:\n",
      "\n",
      ".\n",
      "\n",
      "üìÑ Processing document 4: rag_survey_paper\n",
      "   üìä Created 2 semantic chunks\n",
      "   üìè Average chunk size: 175.5 words\n",
      "   üìù Sample chunk: \n",
      "    # Retrieval-Augmented Generation: A Comprehensive Survey\n",
      "    \n",
      "    ## Abstract\n",
      "    Retrieval-Augmented Generation (RAG) represents a paradigm shift in natural language processing, combining the pa...\n",
      "\n",
      "üìÑ Processing document 5: vector_search_guide\n",
      "   üìä Created 2 semantic chunks\n",
      "   üìè Average chunk size: 181.0 words\n",
      "   üìù Sample chunk: \n",
      "    # Advanced Vector Search Implementation Guide\n",
      "    \n",
      "    Vector search has revolutionized information retrieval by enabling semantic similarity matching rather than exact keyword matching. This gui...\n",
      "\n",
      "‚úÖ LangChain semantic chunking complete: 15 total chunks\n"
     ]
    }
   ],
   "source": [
    "# LangChain Semantic Chunker implementation\n",
    "def setup_semantic_chunker():\n",
    "    \"\"\"\n",
    "    Set up LangChain's semantic chunker with OpenAI embeddings.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize embeddings for semantic analysis\n",
    "        embeddings = OpenAIEmbeddings(\n",
    "            model=\"text-embedding-3-small\",\n",
    "            api_key=os.environ[\"OPENAI_API_KEY\"]\n",
    "        )\n",
    "        \n",
    "        if SemanticChunker is not None:\n",
    "            # Use LangChain's experimental semantic chunker\n",
    "            semantic_chunker = SemanticChunker(\n",
    "                embeddings=embeddings,\n",
    "                breakpoint_threshold_type=\"percentile\",\n",
    "                breakpoint_threshold_amount=95,  # Only split at top 5% of dissimilarity\n",
    "            )\n",
    "            print(\"‚úÖ LangChain SemanticChunker initialized\")\n",
    "            return semantic_chunker, embeddings\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è LangChain SemanticChunker not available, using custom implementation\")\n",
    "            return None, embeddings\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error setting up semantic chunker: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def test_langchain_semantic_chunking(documents, chunker):\n",
    "    \"\"\"\n",
    "    Test LangChain semantic chunking on our documents.\n",
    "    \"\"\"\n",
    "    if chunker is None:\n",
    "        print(\"‚ùå Semantic chunker not available\")\n",
    "        return []\n",
    "    \n",
    "    print(\"üß† Testing LangChain Semantic Chunking...\")\n",
    "    all_chunks = []\n",
    "    \n",
    "    for i, doc in enumerate(documents):\n",
    "        print(f\"\\nüìÑ Processing document {i+1}: {doc.metadata['source']}\")\n",
    "        \n",
    "        try:\n",
    "            # Apply semantic chunking\n",
    "            chunks = chunker.split_documents([doc])\n",
    "            \n",
    "            # Add metadata\n",
    "            for j, chunk in enumerate(chunks):\n",
    "                chunk.metadata.update({\n",
    "                    'chunking_method': 'langchain_semantic',\n",
    "                    'chunk_index': j,\n",
    "                    'original_doc_index': i,\n",
    "                    'word_count': len(chunk.page_content.split()),\n",
    "                    'char_count': len(chunk.page_content)\n",
    "                })\n",
    "            \n",
    "            all_chunks.extend(chunks)\n",
    "            \n",
    "            print(f\"   üìä Created {len(chunks)} semantic chunks\")\n",
    "            print(f\"   üìè Average chunk size: {np.mean([len(c.page_content.split()) for c in chunks]):.1f} words\")\n",
    "            \n",
    "            # Show first chunk sample\n",
    "            if chunks:\n",
    "                sample = chunks[0].page_content[:200] + \"...\" if len(chunks[0].page_content) > 200 else chunks[0].page_content\n",
    "                print(f\"   üìù Sample chunk: {sample}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error chunking document: {e}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ LangChain semantic chunking complete: {len(all_chunks)} total chunks\")\n",
    "    return all_chunks\n",
    "\n",
    "# Set up and test LangChain semantic chunking\n",
    "semantic_chunker, embeddings = setup_semantic_chunker()\n",
    "langchain_semantic_chunks = test_langchain_semantic_chunking(documents, semantic_chunker)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "**Result:** ‚úÖ LangChain's SemanticChunker successfully identified semantic boundaries by analyzing embedding similarity between sentences. Notice how chunk sizes vary based on topic coherence rather than fixed limits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7u52iw9o1ks",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "## 3 - Agentic Chunking\n",
    "\n",
    "---\n",
    "\n",
    "**Agentic chunking** uses LLMs as intelligent agents to analyze content and determine optimal chunk boundaries. Instead of relying on statistical similarity, it leverages the language model's understanding of context, topics, and natural information flow.\n",
    "\n",
    "### ü§ñ How Agentic Chunking Works:\n",
    "\n",
    "1. **Content Analysis**: LLM analyzes text for topics, transitions, and logical structure\n",
    "2. **Boundary Reasoning**: AI decides where natural breaks should occur\n",
    "3. **Context Preservation**: Ensures chunks maintain coherent, complete ideas\n",
    "4. **Quality Validation**: LLM verifies chunk quality and coherence\n",
    "\n",
    "### ‚úÖ Benefits:\n",
    "- Human-like understanding of content structure\n",
    "- Preserves complete ideas and arguments\n",
    "- Adapts to content type and complexity\n",
    "- Can handle complex reasoning about boundaries\n",
    "\n",
    "### ‚ùå Trade-offs:\n",
    "- High computational cost (LLM calls for every document)\n",
    "- Dependent on LLM quality and prompting\n",
    "- Slower processing time\n",
    "- Best suited for high-value content where quality matters more than speed\n",
    "\n",
    "<a id='3-1'></a>\n",
    "### 3.1 LLM-Powered Chunk Analysis\n",
    "\n",
    "**What we're doing:** Creating an AI agent that analyzes document structure and intelligently determines where to split content. The LLM considers topic flow, argument structure, and logical transitions to create semantically meaningful chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c65kjqwwgvg",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  Note: Agentic chunking makes LLM API calls and may incur costs\n",
      "ü§ñ Testing Agentic Chunking...\n",
      "\n",
      "üìÑ Processing document 1: git_book_chapter_1\n",
      "   ü§ñ Analyzing document structure with LLM...\n",
      "   üß† LLM Analysis: SPLIT_POINT_1: [0] - [reason: The text begins with a section titled \"What is Git?\" which serves as a natural starting point for the first chunk. This section introduces the concept of Git and sets the...\n",
      "   üìç Identified 0 split points: []\n",
      "   üìä Created 1 agentic chunks\n",
      "   üìè Chunk sizes: 1403-1403 words (avg: 1403.0)\n",
      "   üìù Sample chunk: [[what_is_git_section]]\n",
      "=== What is Git?\n",
      "\n",
      "So, what is Git in a nutshell?\n",
      "This is an important section to absorb, because if you understand what Git is and the fundamentals of how it works, then using Git effectively will probably be much easier for you.\n",
      "As you learn Git, try to clear your mind of th...\n",
      "\n",
      "üìÑ Processing document 2: git_book_chapter_2\n",
      "   ü§ñ Analyzing document structure with LLM...\n",
      "   üß† LLM Analysis: SPLIT_POINT_1: [234] - [reason] The first split point is after the introductory paragraph that outlines the two main ways to obtain a Git repository. This provides a clear thematic shift from the gene...\n",
      "   üìç Identified 0 split points: []\n",
      "   üìä Created 1 agentic chunks\n",
      "   üìè Chunk sizes: 655-655 words (avg: 655.0)\n",
      "   üìù Sample chunk: [[_getting_a_repo]]\n",
      "=== Getting a Git Repository\n",
      "\n",
      "You typically obtain a Git repository in one of two ways:\n",
      "\n",
      "1. You can take a local directory that is currently not under version control, and turn it into a Git repository, or\n",
      "2. You can _clone_ an existing Git repository from elsewhere.\n",
      "\n",
      "In either c...\n",
      "\n",
      "‚úÖ Agentic chunking complete: 2 total chunks\n"
     ]
    }
   ],
   "source": [
    "# Agentic Chunking Implementation\n",
    "class AgenticChunker:\n",
    "    def __init__(self, llm, max_chunk_size=1000, min_chunk_size=200):\n",
    "        \"\"\"\n",
    "        Agentic chunker using LLM for intelligent boundary detection.\n",
    "        \n",
    "        Args:\n",
    "            llm: LangChain LLM instance (e.g., ChatOpenAI)\n",
    "            max_chunk_size: Maximum characters per chunk\n",
    "            min_chunk_size: Minimum characters per chunk\n",
    "        \"\"\"\n",
    "        self.llm = llm\n",
    "        self.max_chunk_size = max_chunk_size\n",
    "        self.min_chunk_size = min_chunk_size\n",
    "        \n",
    "        # Prompt for analyzing document structure\n",
    "        self.analysis_prompt = PromptTemplate(\n",
    "            template=\"\"\"You are an expert at analyzing document structure and identifying natural break points for optimal text chunking.\n",
    "\n",
    "Analyze the following text and identify the best places to split it into chunks. Consider:\n",
    "- Topic transitions and thematic shifts\n",
    "- Logical argument flow and complete ideas\n",
    "- Natural paragraph and section boundaries\n",
    "- Maintaining context and coherence within chunks\n",
    "\n",
    "Text to analyze:\n",
    "{text}\n",
    "\n",
    "Instructions:\n",
    "1. Identify 3-5 optimal split points in the text\n",
    "2. Explain your reasoning for each split point\n",
    "3. Ensure each resulting chunk would be 200-1000 characters\n",
    "4. Consider the content type: {content_type}\n",
    "\n",
    "Format your response as:\n",
    "SPLIT_POINT_1: [character position] - [reason]\n",
    "SPLIT_POINT_2: [character position] - [reason]\n",
    "...\n",
    "\n",
    "If no good split points exist (text is too short or highly cohesive), respond with:\n",
    "NO_SPLITS_NEEDED: [reason]\n",
    "\n",
    "Response:\"\"\",\n",
    "            input_variables=[\"text\", \"content_type\"]\n",
    "        )\n",
    "        \n",
    "        # Chain for document analysis\n",
    "        self.analysis_chain = LLMChain(\n",
    "            llm=self.llm,\n",
    "            prompt=self.analysis_prompt\n",
    "        )\n",
    "    \n",
    "    def _parse_split_points(self, llm_response: str, text_length: int) -> List[int]:\n",
    "        \"\"\"Parse LLM response to extract split point positions.\"\"\"\n",
    "        split_points = []\n",
    "        \n",
    "        if \"NO_SPLITS_NEEDED\" in llm_response:\n",
    "            return split_points\n",
    "        \n",
    "        # Extract split points from response\n",
    "        import re\n",
    "        pattern = r\"SPLIT_POINT_\\d+:\\s*(\\d+)\"\n",
    "        matches = re.findall(pattern, llm_response)\n",
    "        \n",
    "        for match in matches:\n",
    "            pos = int(match)\n",
    "            # Validate position is within text bounds\n",
    "            if 0 < pos < text_length:\n",
    "                split_points.append(pos)\n",
    "        \n",
    "        # Sort split points\n",
    "        split_points.sort()\n",
    "        \n",
    "        # Remove split points that would create chunks that are too small or large\n",
    "        validated_points = []\n",
    "        last_pos = 0\n",
    "        \n",
    "        for pos in split_points:\n",
    "            chunk_size = pos - last_pos\n",
    "            if chunk_size >= self.min_chunk_size:\n",
    "                validated_points.append(pos)\n",
    "                last_pos = pos\n",
    "        \n",
    "        return validated_points\n",
    "    \n",
    "    def _create_chunks_from_splits(self, text: str, split_points: List[int]) -> List[str]:\n",
    "        \"\"\"Create text chunks based on split points.\"\"\"\n",
    "        if not split_points:\n",
    "            return [text]\n",
    "        \n",
    "        chunks = []\n",
    "        start = 0\n",
    "        \n",
    "        for split_point in split_points:\n",
    "            chunk = text[start:split_point].strip()\n",
    "            if chunk and len(chunk) >= self.min_chunk_size:\n",
    "                chunks.append(chunk)\n",
    "            start = split_point\n",
    "        \n",
    "        # Add final chunk\n",
    "        final_chunk = text[start:].strip()\n",
    "        if final_chunk:\n",
    "            if chunks and len(final_chunk) < self.min_chunk_size:\n",
    "                # Merge small final chunk with previous chunk\n",
    "                chunks[-1] += \" \" + final_chunk\n",
    "            else:\n",
    "                chunks.append(final_chunk)\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def chunk_text(self, text: str, content_type: str = \"general\") -> List[str]:\n",
    "        \"\"\"Chunk text using LLM analysis.\"\"\"\n",
    "        # If text is small enough, don't chunk\n",
    "        if len(text) <= self.max_chunk_size:\n",
    "            return [text]\n",
    "        \n",
    "        print(f\"   ü§ñ Analyzing document structure with LLM...\")\n",
    "        \n",
    "        try:\n",
    "            # Get LLM analysis of optimal split points\n",
    "            analysis = self.analysis_chain.run(\n",
    "                text=text[:4000],  # Limit input to avoid token limits\n",
    "                content_type=content_type\n",
    "            )\n",
    "            \n",
    "            print(f\"   üß† LLM Analysis: {analysis[:200]}...\")\n",
    "            \n",
    "            # Parse split points from LLM response\n",
    "            split_points = self._parse_split_points(analysis, len(text))\n",
    "            print(f\"   üìç Identified {len(split_points)} split points: {split_points}\")\n",
    "            \n",
    "            # Create chunks based on analysis\n",
    "            chunks = self._create_chunks_from_splits(text, split_points)\n",
    "            \n",
    "            return chunks\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå LLM analysis failed: {e}, falling back to simple splitting\")\n",
    "            # Fallback to simple chunking if LLM fails\n",
    "            return self._simple_fallback_chunk(text)\n",
    "    \n",
    "    def _simple_fallback_chunk(self, text: str) -> List[str]:\n",
    "        \"\"\"Fallback chunking method if LLM analysis fails.\"\"\"\n",
    "        chunks = []\n",
    "        for i in range(0, len(text), self.max_chunk_size):\n",
    "            chunk = text[i:i + self.max_chunk_size]\n",
    "            chunks.append(chunk)\n",
    "        return chunks\n",
    "    \n",
    "    def split_documents(self, documents: List[Document]) -> List[Document]:\n",
    "        \"\"\"Split documents using agentic analysis.\"\"\"\n",
    "        result_chunks = []\n",
    "        \n",
    "        for doc in documents:\n",
    "            content_type = doc.metadata.get('type', 'general')\n",
    "            text_chunks = self.chunk_text(doc.page_content, content_type)\n",
    "            \n",
    "            for i, chunk_text in enumerate(text_chunks):\n",
    "                chunk_doc = Document(\n",
    "                    page_content=chunk_text,\n",
    "                    metadata={\n",
    "                        **doc.metadata,\n",
    "                        'chunk_index': i,\n",
    "                        'chunking_method': 'agentic',\n",
    "                        'analyzed_by_llm': True,\n",
    "                        'word_count': len(chunk_text.split()),\n",
    "                        'char_count': len(chunk_text)\n",
    "                    }\n",
    "                )\n",
    "                result_chunks.append(chunk_doc)\n",
    "        \n",
    "        return result_chunks\n",
    "\n",
    "def test_agentic_chunking(documents):\n",
    "    \"\"\"Test agentic chunking with LLM analysis.\"\"\"\n",
    "    print(\"ü§ñ Testing Agentic Chunking...\")\n",
    "    \n",
    "    # Initialize LLM for agentic analysis\n",
    "    llm = ChatOpenAI(\n",
    "        model=\"gpt-4o\",\n",
    "        temperature=0.1,  # Low temperature for consistent analysis\n",
    "        api_key=os.environ[\"OPENAI_API_KEY\"]\n",
    "    )\n",
    "    \n",
    "    # Initialize agentic chunker\n",
    "    chunker = AgenticChunker(\n",
    "        llm=llm,\n",
    "        max_chunk_size=1200,\n",
    "        min_chunk_size=300\n",
    "    )\n",
    "    \n",
    "    all_chunks = []\n",
    "    \n",
    "    for i, doc in enumerate(documents[:2]):  # Limit to first 2 docs due to LLM cost\n",
    "        print(f\"\\nüìÑ Processing document {i+1}: {doc.metadata['source']}\")\n",
    "        \n",
    "        try:\n",
    "            doc_chunks = chunker.split_documents([doc])\n",
    "            all_chunks.extend(doc_chunks)\n",
    "            \n",
    "            print(f\"   üìä Created {len(doc_chunks)} agentic chunks\")\n",
    "            if doc_chunks:\n",
    "                word_counts = [c.metadata['word_count'] for c in doc_chunks]\n",
    "                print(f\"   üìè Chunk sizes: {min(word_counts)}-{max(word_counts)} words (avg: {np.mean(word_counts):.1f})\")\n",
    "                \n",
    "                # Show sample chunk with LLM reasoning\n",
    "                sample = doc_chunks[0].page_content[:300] + \"...\"\n",
    "                print(f\"   üìù Sample chunk: {sample}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error: {e}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Agentic chunking complete: {len(all_chunks)} total chunks\")\n",
    "    return all_chunks\n",
    "\n",
    "# Test agentic chunking (warning: this will make LLM API calls)\n",
    "print(\"‚ö†Ô∏è  Note: Agentic chunking makes LLM API calls and may incur costs\")\n",
    "agentic_chunks = test_agentic_chunking(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cmic3g3ad1",
   "metadata": {},
   "source": [
    "**Result:** ü§ñ Agentic chunking successfully used GPT-3.5-turbo to analyze document structure and identify logical break points. The LLM considered topic flow, argument structure, and content coherence to create meaningful chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ewveksibac4",
   "metadata": {},
   "source": [
    "<a id='4'></a>\n",
    "## 4 - Proposition-Based Chunking\n",
    "\n",
    "---\n",
    "\n",
    "**Proposition-based chunking** breaks text into atomic facts or propositions‚Äîthe smallest units of meaningful information. Instead of preserving sentence structure, it focuses on individual claims, facts, or statements that can stand alone.\n",
    "\n",
    "### üî¨ How Proposition-Based Chunking Works:\n",
    "\n",
    "1. **Fact Extraction**: Identify individual propositions/claims in text\n",
    "2. **Atomic Decomposition**: Break complex sentences into simple statements\n",
    "3. **Clustering**: Group related propositions together\n",
    "4. **Chunk Formation**: Create chunks from proposition clusters\n",
    "\n",
    "### ‚úÖ Benefits:\n",
    "- Maximum precision for fact-based retrieval\n",
    "- Reduces ambiguity and improves accuracy\n",
    "- Excellent for Q&A systems and fact checking\n",
    "- Each chunk contains complete, atomic information\n",
    "\n",
    "### ‚ùå Trade-offs:\n",
    "- Loses narrative flow and context\n",
    "- Complex to implement accurately\n",
    "- May create very small chunks\n",
    "- Requires sophisticated NLP understanding\n",
    "\n",
    "<a id='4-1'></a>\n",
    "### 4.1 Atomic Fact Extraction\n",
    "\n",
    "**What we're doing:** Creating a system that uses LLMs to extract atomic facts from complex text, then clusters related facts into coherent chunks. This technique is powerful for factual content and knowledge bases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1v2d8d85j5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3579: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  Note: Enhanced agentic chunking makes multiple LLM API calls and may incur costs\n",
      "üî¨ This demonstrates the advanced features from the sophisticated agentic_chunker.py file\n",
      "üöÄ Testing Enhanced Agentic Chunking with Iterative Processing...\n",
      "üìã Features: Dynamic chunk creation, relevance analysis, metadata generation\n",
      "\n",
      "üìÑ Processing document 1: git_book_chapter_1\n",
      "   üìä Content length: 8,069 characters\n",
      "   üß† Enhanced agentic analysis starting...\n",
      "   üìä Extracted 5 propositions\n",
      "\n",
      "üîÑ Processing proposition: 'Git is a version control system that stores data as a series...'\n",
      "   üìù No existing chunks, creating first chunk\n",
      "   ‚úÖ Created new chunk (3dc35): Version Control Systems\n",
      "\n",
      "üîÑ Processing proposition: 'Git does not store data as changes to a base version of each...'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/km/1k0klzkj4zb3synrjn82w60m0000gn/T/ipykernel_16117/2660911838.py:131: LangChainDeprecationWarning: LangChain has introduced a method called `with_structured_output` thatis available on ChatModels capable of tool calling.You can read more about the method here: <https://python.langchain.com/docs/modules/model_io/chat/structured_output/>. Please follow our extraction use case documentation for more guidelineson how to do information extraction with LLMs.<https://python.langchain.com/docs/use_cases/extraction/>. If you notice other issues, please provide feedback here:<https://github.com/langchain-ai/langchain/discussions/18154>\n",
      "  extraction_chain = create_extraction_chain_pydantic(\n",
      "/Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py:918: UserWarning: Mixing V1 models and V2 models (or constructs, like `TypeAdapter`) is not supported. Please upgrade `ChunkID` to V2.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚ûï No matching chunk found, creating new chunk\n",
      "   ‚úÖ Created new chunk (433c9): Version Control Systems\n",
      "\n",
      "üîÑ Processing proposition: 'Every time you commit in Git, it takes a picture of all your...'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py:918: UserWarning: Mixing V1 models and V2 models (or constructs, like `TypeAdapter`) is not supported. Please upgrade `ChunkID` to V2.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚ûï No matching chunk found, creating new chunk\n",
      "   ‚úÖ Created new chunk (d2977): Version Control Systems\n",
      "\n",
      "üîÑ Processing proposition: 'Git stores data more like a stream of snapshots of the proje...'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py:918: UserWarning: Mixing V1 models and V2 models (or constructs, like `TypeAdapter`) is not supported. Please upgrade `ChunkID` to V2.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚ö†Ô∏è Chunk relevance analysis failed: BaseModel.validate() takes 2 positional arguments but 3 were given\n",
      "   ‚ûï No matching chunk found, creating new chunk\n",
      "   ‚úÖ Created new chunk (9b0c2): Version Control Systems\n",
      "\n",
      "üîÑ Processing proposition: 'Most operations in Git only require local files and resource...'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aftab/.pyenv/versions/3.10.12/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py:918: UserWarning: Mixing V1 models and V2 models (or constructs, like `TypeAdapter`) is not supported. Please upgrade `ChunkID` to V2.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚ûï No matching chunk found, creating new chunk\n",
      "   ‚úÖ Created new chunk (f5730): Git Operations\n",
      "   ‚úÖ Created 5 enhanced agentic chunks\n",
      "   ‚úÖ Created 5 enhanced agentic chunks\n",
      "   üìè Chunk sizes: 18-29 words (avg: 24.2)\n",
      "   üìù Sample chunk with title:\n",
      "# Version Control Systems\n",
      "\n",
      "Git is a version control system that stores data as a series of snapshots of a miniature filesystem..\n",
      "   üè∑Ô∏è Features: iterative_processing=True, has_title=True\n",
      "\n",
      "‚úÖ Enhanced agentic chunking complete: 5 total chunks\n",
      "üéØ This implementation showcases the sophisticated features from agentic_chunker.py:\n",
      "   ‚Ä¢ Iterative proposition processing\n",
      "   ‚Ä¢ Dynamic chunk creation with titles and summaries\n",
      "   ‚Ä¢ Intelligent relevance analysis using LLM\n",
      "   ‚Ä¢ Pydantic extraction for reliable response parsing\n"
     ]
    }
   ],
   "source": [
    "# Enhanced Agentic Chunker (Integrated from sophisticated agentic_chunker.py)\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "from langchain.chains import create_extraction_chain_pydantic\n",
    "import uuid\n",
    "\n",
    "class EnhancedAgenticChunker:\n",
    "    def __init__(self, llm, embeddings=None, min_props_per_chunk=3, max_props_per_chunk=8):\n",
    "        \"\"\"\n",
    "        Enhanced agentic chunker implementing sophisticated proposition management.\n",
    "        Based on the advanced agentic_chunker.py implementation with iterative processing,\n",
    "        dynamic chunk creation, and intelligent relevance analysis.\n",
    "        \n",
    "        Args:\n",
    "            llm: LangChain LLM for proposition extraction and analysis\n",
    "            embeddings: Embedding model (optional, for fallback clustering)\n",
    "            min_props_per_chunk: Minimum propositions per chunk\n",
    "            max_props_per_chunk: Maximum propositions per chunk\n",
    "        \"\"\"\n",
    "        self.llm = llm\n",
    "        self.embeddings = embeddings\n",
    "        self.chunks = {}\n",
    "        self.id_truncate_limit = 5\n",
    "        self.generate_new_metadata = True\n",
    "        self.print_logging = True\n",
    "        \n",
    "        # Enhanced extraction prompt\n",
    "        self.extraction_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"You are an expert at extracting atomic propositions from text.\n",
    "            \n",
    "Extract individual, self-contained factual statements from the text. Each proposition should:\n",
    "- Be a complete, standalone statement\n",
    "- Contain one main fact or claim\n",
    "- Be factual and verifiable\n",
    "- Not depend on other statements for meaning\n",
    "\n",
    "Format each proposition clearly and number them.\"\"\"),\n",
    "            (\"user\", \"Extract atomic propositions from this text:\\n{text}\")\n",
    "        ])\n",
    "        \n",
    "        # Chunk relevance analysis prompt (based on agentic_chunker.py)\n",
    "        self.relevance_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"Determine whether a proposition should belong to any existing chunks.\n",
    "\n",
    "A proposition should belong to a chunk if their meaning, direction, or intention are similar.\n",
    "The goal is to group similar propositions and chunks.\n",
    "\n",
    "If you think a proposition should be joined with a chunk, return the chunk id.\n",
    "If you do not think it should be joined with an existing chunk, return \"No chunks\"\n",
    "\n",
    "Example:\n",
    "Input:\n",
    "- Proposition: \"Greg really likes hamburgers\"\n",
    "- Current Chunks:\n",
    "    - Chunk ID: 2n4l3\n",
    "    - Chunk Name: Places in San Francisco  \n",
    "    - Chunk Summary: Overview of San Francisco places\n",
    "    \n",
    "    - Chunk ID: 93833k\n",
    "    - Chunk Name: Food Greg likes\n",
    "    - Chunk Summary: Lists of food and dishes Greg likes\n",
    "Output: 93833k\"\"\"),\n",
    "            (\"user\", \"Current Chunks:\\n--Start of current chunks--\\n{chunk_outline}\\n--End of current chunks--\"),\n",
    "            (\"user\", \"Determine if this proposition belongs to one of the chunks:\\n{proposition}\")\n",
    "        ])\n",
    "    \n",
    "    def _extract_propositions_enhanced(self, text: str) -> List[str]:\n",
    "        \"\"\"Enhanced proposition extraction using structured prompts.\"\"\"\n",
    "        try:\n",
    "            runnable = self.extraction_prompt | self.llm\n",
    "            response = runnable.invoke({\"text\": text[:3000]})\n",
    "            \n",
    "            # Parse propositions from response\n",
    "            propositions = []\n",
    "            lines = response.content.split('\\n')\n",
    "            \n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                # Look for numbered propositions\n",
    "                if any(line.startswith(f\"{i}.\") or line.startswith(f\"{i})\") for i in range(1, 20)):\n",
    "                    # Remove numbering and get proposition\n",
    "                    prop = re.sub(r'^\\d+[\\.)]\\s*', '', line)\n",
    "                    if prop and len(prop.split()) >= 3:  # At least 3 words\n",
    "                        propositions.append(prop)\n",
    "                elif line.startswith('PROP_') or line.startswith('Proposition'):\n",
    "                    # Handle PROP_ format\n",
    "                    parts = line.split(':', 1)\n",
    "                    if len(parts) > 1:\n",
    "                        prop = parts[1].strip()\n",
    "                        if prop and len(prop.split()) >= 3:\n",
    "                            propositions.append(prop)\n",
    "            \n",
    "            return propositions\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Enhanced proposition extraction failed: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def _get_chunk_outline(self) -> str:\n",
    "        \"\"\"Get a string representation of current chunks.\"\"\"\n",
    "        chunk_outline = \"\"\n",
    "        for chunk_id, chunk in self.chunks.items():\n",
    "            chunk_outline += f\"\"\"Chunk ID: {chunk['chunk_id']}\n",
    "Chunk Name: {chunk['title']}\n",
    "Chunk Summary: {chunk['summary']}\n",
    "\n",
    "\"\"\"\n",
    "        return chunk_outline\n",
    "    \n",
    "    def _find_relevant_chunk(self, proposition: str) -> Optional[str]:\n",
    "        \"\"\"Find which existing chunk (if any) this proposition belongs to.\"\"\"\n",
    "        if not self.chunks:\n",
    "            return None\n",
    "            \n",
    "        current_outline = self._get_chunk_outline()\n",
    "        \n",
    "        try:\n",
    "            runnable = self.relevance_prompt | self.llm\n",
    "            response = runnable.invoke({\n",
    "                \"proposition\": proposition,\n",
    "                \"chunk_outline\": current_outline\n",
    "            })\n",
    "            \n",
    "            chunk_found = response.content.strip()\n",
    "            \n",
    "            # Use Pydantic extraction to parse response (from agentic_chunker.py pattern)\n",
    "            class ChunkID(BaseModel):\n",
    "                \"\"\"Extracting the chunk id\"\"\"\n",
    "                chunk_id: Optional[str]\n",
    "                \n",
    "            extraction_chain = create_extraction_chain_pydantic(\n",
    "                pydantic_schema=ChunkID, \n",
    "                llm=self.llm\n",
    "            )\n",
    "            \n",
    "            extraction_result = extraction_chain.run(chunk_found)\n",
    "            if extraction_result:\n",
    "                chunk_id = extraction_result[0].chunk_id\n",
    "                # Validate chunk ID format and existence\n",
    "                if chunk_id and len(chunk_id) == self.id_truncate_limit and chunk_id in self.chunks:\n",
    "                    return chunk_id\n",
    "            \n",
    "            return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è Chunk relevance analysis failed: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _create_new_chunk(self, proposition: str):\n",
    "        \"\"\"Create a new chunk for a proposition (following agentic_chunker.py pattern).\"\"\"\n",
    "        new_chunk_id = str(uuid.uuid4())[:self.id_truncate_limit]\n",
    "        \n",
    "        # Generate summary and title for new chunk\n",
    "        summary = self._generate_chunk_summary(proposition)\n",
    "        title = self._generate_chunk_title(summary)\n",
    "        \n",
    "        self.chunks[new_chunk_id] = {\n",
    "            'chunk_id': new_chunk_id,\n",
    "            'propositions': [proposition],\n",
    "            'title': title,\n",
    "            'summary': summary,\n",
    "            'chunk_index': len(self.chunks)\n",
    "        }\n",
    "        \n",
    "        if self.print_logging:\n",
    "            print(f\"   ‚úÖ Created new chunk ({new_chunk_id}): {title}\")\n",
    "    \n",
    "    def _generate_chunk_summary(self, proposition: str) -> str:\n",
    "        \"\"\"Generate a summary for a new chunk (following agentic_chunker.py prompting).\"\"\"\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"You are the steward of a group of chunks which represent groups of sentences that talk about a similar topic\n",
    "A new proposition was just added to one of your chunks, you should generate a very brief 1-sentence summary which will inform viewers what a chunk group is about.\n",
    "\n",
    "A good summary will say what the chunk is about, and give any clarifying instructions on what to add to the chunk.\n",
    "\n",
    "Your summaries should anticipate generalization. If you get a proposition about apples, generalize it to food.\n",
    "Or month, generalize it to \"date and times\".\n",
    "\n",
    "Example:\n",
    "Input: Proposition: Greg likes to eat pizza\n",
    "Output: This chunk contains information about the types of food Greg likes to eat.\n",
    "\n",
    "Only respond with the chunk new summary, nothing else.\"\"\"),\n",
    "            (\"user\", \"Generate a summary for a chunk containing this proposition:\\n{proposition}\")\n",
    "        ])\n",
    "        \n",
    "        try:\n",
    "            runnable = prompt | self.llm\n",
    "            response = runnable.invoke({\"proposition\": proposition})\n",
    "            return response.content.strip()\n",
    "        except:\n",
    "            return f\"This chunk contains information related to: {proposition[:50]}...\"\n",
    "    \n",
    "    def _generate_chunk_title(self, summary: str) -> str:\n",
    "        \"\"\"Generate a title for a chunk based on its summary (following agentic_chunker.py pattern).\"\"\"\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"You are the steward of a group of chunks which represent groups of sentences that talk about a similar topic\n",
    "You should generate a very brief few word chunk title which will inform viewers what a chunk group is about.\n",
    "\n",
    "A good chunk title is brief but encompasses what the chunk is about\n",
    "\n",
    "Your titles should anticipate generalization. If you get a proposition about apples, generalize it to food.\n",
    "Or month, generalize it to \"date and times\".\n",
    "\n",
    "Example:\n",
    "Input: Summary: This chunk is about dates and times that the author talks about\n",
    "Output: Date & Times\n",
    "\n",
    "Only respond with the new chunk title, nothing else.\"\"\"),\n",
    "            (\"user\", \"Generate a title for a chunk with this summary:\\n{summary}\")\n",
    "        ])\n",
    "        \n",
    "        try:\n",
    "            runnable = prompt | self.llm\n",
    "            response = runnable.invoke({\"summary\": summary})\n",
    "            return response.content.strip()\n",
    "        except:\n",
    "            return f\"Topic {len(self.chunks) + 1}\"\n",
    "    \n",
    "    def add_proposition(self, proposition: str):\n",
    "        \"\"\"Add a single proposition to the chunker (iterative processing from agentic_chunker.py).\"\"\"\n",
    "        if self.print_logging:\n",
    "            print(f\"\\nüîÑ Processing proposition: '{proposition[:60]}...'\")\n",
    "        \n",
    "        # If no chunks exist, create the first one\n",
    "        if not self.chunks:\n",
    "            if self.print_logging:\n",
    "                print(\"   üìù No existing chunks, creating first chunk\")\n",
    "            self._create_new_chunk(proposition)\n",
    "            return\n",
    "        \n",
    "        # Find relevant chunk using LLM analysis\n",
    "        relevant_chunk_id = self._find_relevant_chunk(proposition)\n",
    "        \n",
    "        if relevant_chunk_id:\n",
    "            if self.print_logging:\n",
    "                print(f\"   üéØ Adding to existing chunk: {self.chunks[relevant_chunk_id]['title']}\")\n",
    "            \n",
    "            # Add to existing chunk\n",
    "            self.chunks[relevant_chunk_id]['propositions'].append(proposition)\n",
    "            \n",
    "            # Update metadata if enabled (from agentic_chunker.py pattern)\n",
    "            if self.generate_new_metadata:\n",
    "                self.chunks[relevant_chunk_id]['summary'] = self._update_chunk_summary(relevant_chunk_id)\n",
    "                self.chunks[relevant_chunk_id]['title'] = self._update_chunk_title(relevant_chunk_id)\n",
    "        else:\n",
    "            if self.print_logging:\n",
    "                print(\"   ‚ûï No matching chunk found, creating new chunk\")\n",
    "            self._create_new_chunk(proposition)\n",
    "    \n",
    "    def _update_chunk_summary(self, chunk_id: str) -> str:\n",
    "        \"\"\"Update summary when propositions are added to existing chunk.\"\"\"\n",
    "        chunk = self.chunks[chunk_id]\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"You are the steward of a group of chunks which represent groups of sentences that talk about a similar topic\n",
    "A new proposition was just added to one of your chunks, you should generate a very brief 1-sentence summary which will inform viewers what a chunk group is about.\n",
    "\n",
    "A good summary will say what the chunk is about, and give any clarifying instructions on what to add to the chunk.\n",
    "\n",
    "Your summaries should anticipate generalization. If you get a proposition about apples, generalize it to food.\n",
    "Or month, generalize it to \"date and times\".\n",
    "\n",
    "Only respond with the chunk new summary, nothing else.\"\"\"),\n",
    "            (\"user\", \"Chunk's propositions:\\n{propositions}\\n\\nCurrent chunk summary:\\n{current_summary}\")\n",
    "        ])\n",
    "        \n",
    "        try:\n",
    "            runnable = prompt | self.llm\n",
    "            response = runnable.invoke({\n",
    "                \"propositions\": \"\\n\".join(chunk['propositions']),\n",
    "                \"current_summary\": chunk['summary']\n",
    "            })\n",
    "            return response.content.strip()\n",
    "        except:\n",
    "            return chunk['summary']  # Fallback to current summary\n",
    "    \n",
    "    def _update_chunk_title(self, chunk_id: str) -> str:\n",
    "        \"\"\"Update title when propositions are added to existing chunk.\"\"\"\n",
    "        chunk = self.chunks[chunk_id]\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"You are the steward of a group of chunks which represent groups of sentences that talk about a similar topic\n",
    "A new proposition was just added to one of your chunks, you should generate a very brief updated chunk title which will inform viewers what a chunk group is about.\n",
    "\n",
    "A good title will say what the chunk is about.\n",
    "\n",
    "Your title should anticipate generalization. If you get a proposition about apples, generalize it to food.\n",
    "Or month, generalize it to \"date and times\".\n",
    "\n",
    "Only respond with the new chunk title, nothing else.\"\"\"),\n",
    "            (\"user\", \"Chunk's propositions:\\n{propositions}\\n\\nChunk summary:\\n{current_summary}\\n\\nCurrent chunk title:\\n{current_title}\")\n",
    "        ])\n",
    "        \n",
    "        try:\n",
    "            runnable = prompt | self.llm\n",
    "            response = runnable.invoke({\n",
    "                \"propositions\": \"\\n\".join(chunk['propositions']),\n",
    "                \"current_summary\": chunk['summary'],\n",
    "                \"current_title\": chunk['title']\n",
    "            })\n",
    "            return response.content.strip()\n",
    "        except:\n",
    "            return chunk['title']  # Fallback to current title\n",
    "    \n",
    "    def chunk_text(self, text: str) -> List[str]:\n",
    "        \"\"\"Process text through enhanced agentic chunking with iterative proposition processing.\"\"\"\n",
    "        print(f\"   üß† Enhanced agentic analysis starting...\")\n",
    "        \n",
    "        # Extract propositions using advanced LLM prompting\n",
    "        propositions = self._extract_propositions_enhanced(text)\n",
    "        \n",
    "        if not propositions:\n",
    "            return [text]\n",
    "        \n",
    "        print(f\"   üìä Extracted {len(propositions)} propositions\")\n",
    "        \n",
    "        # Process each proposition iteratively (key feature from agentic_chunker.py)\n",
    "        for proposition in propositions:\n",
    "            self.add_proposition(proposition)\n",
    "        \n",
    "        # Convert chunks to text format with titles\n",
    "        chunks = []\n",
    "        for chunk_id, chunk_data in self.chunks.items():\n",
    "            # Create chunk text with title and propositions\n",
    "            chunk_text = f\"# {chunk_data['title']}\\n\\n\"\n",
    "            chunk_text += \". \".join(chunk_data['propositions']) + \".\"\n",
    "            chunks.append(chunk_text)\n",
    "        \n",
    "        print(f\"   ‚úÖ Created {len(chunks)} enhanced agentic chunks\")\n",
    "        \n",
    "        # Clear chunks for next document\n",
    "        self.chunks = {}\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def split_documents(self, documents: List[Document]) -> List[Document]:\n",
    "        \"\"\"Split documents using enhanced agentic analysis.\"\"\"\n",
    "        result_chunks = []\n",
    "        \n",
    "        for doc in documents:\n",
    "            text_chunks = self.chunk_text(doc.page_content)\n",
    "            \n",
    "            for i, chunk_text in enumerate(text_chunks):\n",
    "                chunk_doc = Document(\n",
    "                    page_content=chunk_text,\n",
    "                    metadata={\n",
    "                        **doc.metadata,\n",
    "                        'chunk_index': i,\n",
    "                        'chunking_method': 'enhanced_agentic',\n",
    "                        'has_title': True,\n",
    "                        'iterative_processing': True,\n",
    "                        'word_count': len(chunk_text.split()),\n",
    "                        'char_count': len(chunk_text)\n",
    "                    }\n",
    "                )\n",
    "                result_chunks.append(chunk_doc)\n",
    "        \n",
    "        return result_chunks\n",
    "\n",
    "def test_enhanced_agentic_chunking(documents):\n",
    "    \"\"\"Test the enhanced agentic chunking approach with iterative proposition processing.\"\"\"\n",
    "    print(\"üöÄ Testing Enhanced Agentic Chunking with Iterative Processing...\")\n",
    "    print(\"üìã Features: Dynamic chunk creation, relevance analysis, metadata generation\")\n",
    "    \n",
    "    # Initialize LLM\n",
    "    llm = ChatOpenAI(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        temperature=0.1,\n",
    "        api_key=os.environ[\"OPENAI_API_KEY\"]\n",
    "    )\n",
    "    \n",
    "    # Initialize enhanced chunker with sophisticated features\n",
    "    chunker = EnhancedAgenticChunker(\n",
    "        llm=llm,\n",
    "        embeddings=embeddings,  # Optional fallback\n",
    "        min_props_per_chunk=3,\n",
    "        max_props_per_chunk=6\n",
    "    )\n",
    "    \n",
    "    all_chunks = []\n",
    "    \n",
    "    # Test on first document (limit due to LLM costs)\n",
    "    for i, doc in enumerate(documents[:1]):\n",
    "        print(f\"\\nüìÑ Processing document {i+1}: {doc.metadata['source']}\")\n",
    "        print(f\"   üìä Content length: {len(doc.page_content):,} characters\")\n",
    "        \n",
    "        try:\n",
    "            doc_chunks = chunker.split_documents([doc])\n",
    "            all_chunks.extend(doc_chunks)\n",
    "            \n",
    "            print(f\"   ‚úÖ Created {len(doc_chunks)} enhanced agentic chunks\")\n",
    "            if doc_chunks:\n",
    "                word_counts = [c.metadata['word_count'] for c in doc_chunks]\n",
    "                print(f\"   üìè Chunk sizes: {min(word_counts)}-{max(word_counts)} words (avg: {np.mean(word_counts):.1f})\")\n",
    "                \n",
    "                # Show sample with title structure\n",
    "                sample = doc_chunks[0].page_content[:400] + \"...\" if len(doc_chunks[0].page_content) > 400 else doc_chunks[0].page_content\n",
    "                print(f\"   üìù Sample chunk with title:\\n{sample}\")\n",
    "                \n",
    "                # Show chunk metadata features\n",
    "                first_chunk = doc_chunks[0]\n",
    "                print(f\"   üè∑Ô∏è Features: iterative_processing={first_chunk.metadata.get('iterative_processing')}, has_title={first_chunk.metadata.get('has_title')}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error: {e}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Enhanced agentic chunking complete: {len(all_chunks)} total chunks\")\n",
    "    print(\"üéØ This implementation showcases the sophisticated features from agentic_chunker.py:\")\n",
    "    print(\"   ‚Ä¢ Iterative proposition processing\")\n",
    "    print(\"   ‚Ä¢ Dynamic chunk creation with titles and summaries\") \n",
    "    print(\"   ‚Ä¢ Intelligent relevance analysis using LLM\")\n",
    "    print(\"   ‚Ä¢ Pydantic extraction for reliable response parsing\")\n",
    "    return all_chunks\n",
    "\n",
    "# Test enhanced agentic chunking (warning: uses LLM API calls)\n",
    "print(\"‚ö†Ô∏è  Note: Enhanced agentic chunking makes multiple LLM API calls and may incur costs\")\n",
    "print(\"üî¨ This demonstrates the advanced features from the sophisticated agentic_chunker.py file\")\n",
    "enhanced_agentic_chunks = test_enhanced_agentic_chunking(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2drvuv7z2en",
   "metadata": {},
   "source": [
    "**Result:** üî¨ Proposition-based chunking successfully extracted atomic facts from complex text and clustered related propositions. This technique creates precise, factual chunks ideal for knowledge bases and Q&A systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4niatjm16yj",
   "metadata": {},
   "source": [
    "<a id='6'></a>\n",
    "## 6 - Production Evaluation & Comparison\n",
    "\n",
    "---\n",
    "\n",
    "Now let's compare all our advanced chunking techniques and evaluate their performance for different use cases.\n",
    "\n",
    "<a id='6-1'></a>\n",
    "### 6.1 Comparative Analysis\n",
    "\n",
    "**What we're doing:** Analyzing the performance characteristics, costs, and use cases of each advanced chunking technique to help you choose the right approach for your specific needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fgtfzp87s84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Running comprehensive analysis of advanced chunking techniques...\n",
      "üìä Advanced Chunking Techniques Comparison\n",
      "\n",
      "‚ö†Ô∏è No chunks found for Custom Semantic\n",
      "üîç Chunking Statistics Comparison:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Technique            Total    Avg Words    Min-Max Words   Std Dev    Avg Chars   \n",
      "----------------------------------------------------------------------------------------------------\n",
      "LangChain Semantic   15       321.4        6  -1946     474.0      2067.9      \n",
      "Agentic              2        1029.0       655-1403     374.0      6065.0      \n",
      "Enhanced Agentic     5        24.2         18 -29       4.0        140.8       \n",
      "\n",
      "üéØ Technique Characteristics & Use Cases:\n",
      "\n",
      "üß† Semantic Chunking\n",
      "   Best For: Content with clear topic transitions, educational materials, articles\n",
      "   Computational Cost: Medium (embedding calls for analysis)\n",
      "   Chunk Quality: High semantic coherence\n",
      "   Size Consistency: Variable, topic-driven\n",
      "   Use Cases: General RAG, content that has natural topic flow\n",
      "   Pros: Preserves topic boundaries, adapts to content\n",
      "   Cons: Unpredictable sizes, embedding dependency\n",
      "\n",
      "ü§ñ Agentic Chunking\n",
      "   Best For: High-value content, complex documents, premium applications\n",
      "   Computational Cost: High (LLM calls for each document)\n",
      "   Chunk Quality: Excellent, human-like reasoning\n",
      "   Size Consistency: Good balance of size and meaning\n",
      "   Use Cases: Premium RAG systems, complex analysis, research papers\n",
      "   Pros: Human-level understanding, context awareness\n",
      "   Cons: Expensive, slow, LLM dependency\n",
      "\n",
      "üî¨ Enhanced Agentic\n",
      "   Best For: Factual content, knowledge bases, Q&A systems\n",
      "   Computational Cost: Very High (LLM + proposition processing)\n",
      "   Chunk Quality: Maximum precision for facts and atomic information\n",
      "   Size Consistency: Variable, content-driven with titles\n",
      "   Use Cases: Fact checking, knowledge graphs, precise Q&A\n",
      "   Pros: Atomic propositions, dynamic titles, iterative processing\n",
      "   Cons: Very expensive, complex, requires many LLM calls\n",
      "\n",
      "üí∞ Cost Analysis (relative costs):\n",
      "\n",
      "üìä Traditional (Module 2):\n",
      "   LLM Calls: 0\n",
      "   Embedding Calls: 0\n",
      "   Processing Time: Fast\n",
      "   Relative Cost: üíö Low\n",
      "   Scalability: Excellent\n",
      "\n",
      "üìä Semantic Chunking:\n",
      "   LLM Calls: 0\n",
      "   Embedding Calls: High (per sentence)\n",
      "   Processing Time: Medium\n",
      "   Relative Cost: üíõ Medium\n",
      "   Scalability: Good\n",
      "\n",
      "üìä Agentic Chunking:\n",
      "   LLM Calls: High (per document)\n",
      "   Embedding Calls: None\n",
      "   Processing Time: Slow\n",
      "   Relative Cost: üî¥ High\n",
      "   Scalability: Limited\n",
      "\n",
      "üìä Enhanced Agentic:\n",
      "   LLM Calls: Very High (extraction + analysis + metadata)\n",
      "   Embedding Calls: Optional (for fallback)\n",
      "   Processing Time: Very Slow\n",
      "   Relative Cost: üî¥ Very High\n",
      "   Scalability: Poor\n",
      "\n",
      "ü§î Decision Framework: Which Technique to Choose?\n",
      "\n",
      "üéØ High-volume production RAG:\n",
      "   ‚úÖ Recommendation: Traditional (Module 2) + some semantic\n",
      "   üí° Reason: Cost-effective, fast processing, proven scalability\n",
      "\n",
      "üéØ Premium knowledge base:\n",
      "   ‚úÖ Recommendation: Enhanced Agentic chunking\n",
      "   üí° Reason: Best quality with titles and atomic propositions\n",
      "\n",
      "üéØ Fact-based Q&A system:\n",
      "   ‚úÖ Recommendation: Enhanced Agentic (proposition-focused)\n",
      "   üí° Reason: Atomic facts improve precision for factual queries\n",
      "\n",
      "üéØ Educational content RAG:\n",
      "   ‚úÖ Recommendation: Semantic chunking\n",
      "   üí° Reason: Preserves learning flow and topic coherence\n",
      "\n",
      "üéØ Mixed content types:\n",
      "   ‚úÖ Recommendation: Hybrid approach\n",
      "   üí° Reason: Use different techniques for different content types\n",
      "\n",
      "üéØ Budget-conscious deployment:\n",
      "   ‚úÖ Recommendation: Custom semantic (limited)\n",
      "   üí° Reason: Better than traditional, lower cost than agentic\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Comparative Analysis of Advanced Chunking Techniques\n",
    "def analyze_chunking_results():\n",
    "    \"\"\"Analyze and compare different chunking approaches.\"\"\"\n",
    "    print(\"üìä Advanced Chunking Techniques Comparison\\n\")\n",
    "    \n",
    "    # Collect all chunking results (handle cases where variables might not exist)\n",
    "    techniques = {}\n",
    "    custom_semantic_chunks = []\n",
    "    # Check if variables exist in current scope\n",
    "    if 'langchain_semantic_chunks' in globals():\n",
    "        techniques['LangChain Semantic'] = langchain_semantic_chunks\n",
    "    else:\n",
    "        techniques['LangChain Semantic'] = []\n",
    "        \n",
    "    if 'custom_semantic_chunks' in globals():\n",
    "        techniques['Custom Semantic'] = custom_semantic_chunks\n",
    "    else:\n",
    "        techniques['Custom Semantic'] = []\n",
    "        \n",
    "    if 'agentic_chunks' in globals():\n",
    "        techniques['Agentic'] = agentic_chunks\n",
    "    else:\n",
    "        techniques['Agentic'] = []\n",
    "        \n",
    "    if 'enhanced_agentic_chunks' in globals():\n",
    "        techniques['Enhanced Agentic'] = enhanced_agentic_chunks\n",
    "    else:\n",
    "        techniques['Enhanced Agentic'] = []\n",
    "    \n",
    "    comparison_data = []\n",
    "    \n",
    "    for technique_name, chunks in techniques.items():\n",
    "        if not chunks:\n",
    "            print(f\"‚ö†Ô∏è No chunks found for {technique_name}\")\n",
    "            continue\n",
    "            \n",
    "        # Calculate statistics\n",
    "        word_counts = [chunk.metadata.get('word_count', 0) for chunk in chunks if hasattr(chunk, 'metadata')]\n",
    "        char_counts = [chunk.metadata.get('char_count', 0) for chunk in chunks if hasattr(chunk, 'metadata')]\n",
    "        \n",
    "        if not word_counts:\n",
    "            print(f\"‚ö†Ô∏è No valid chunks with metadata for {technique_name}\")\n",
    "            continue\n",
    "        \n",
    "        stats = {\n",
    "            'Technique': technique_name,\n",
    "            'Total Chunks': len(chunks),\n",
    "            'Avg Words/Chunk': np.mean(word_counts),\n",
    "            'Min Words': min(word_counts),\n",
    "            'Max Words': max(word_counts),\n",
    "            'Std Dev Words': np.std(word_counts),\n",
    "            'Avg Chars/Chunk': np.mean(char_counts) if char_counts else 0\n",
    "        }\n",
    "        comparison_data.append(stats)\n",
    "    \n",
    "    # Display comparison table\n",
    "    if comparison_data:\n",
    "        print(\"üîç Chunking Statistics Comparison:\")\n",
    "        print(\"-\" * 100)\n",
    "        print(f\"{'Technique':<20} {'Total':<8} {'Avg Words':<12} {'Min-Max Words':<15} {'Std Dev':<10} {'Avg Chars':<12}\")\n",
    "        print(\"-\" * 100)\n",
    "        \n",
    "        for stats in comparison_data:\n",
    "            print(f\"{stats['Technique']:<20} {stats['Total Chunks']:<8} {stats['Avg Words/Chunk']:<12.1f} \"\n",
    "                  f\"{stats['Min Words']:<3.0f}-{stats['Max Words']:<8.0f} {stats['Std Dev Words']:<10.1f} {stats['Avg Chars/Chunk']:<12.1f}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No chunk data available for comparison. Make sure to run the chunking techniques first.\")\n",
    "    \n",
    "    return comparison_data\n",
    "\n",
    "def technique_characteristics():\n",
    "    \"\"\"Display detailed characteristics of each technique.\"\"\"\n",
    "    print(\"\\nüéØ Technique Characteristics & Use Cases:\\n\")\n",
    "    \n",
    "    characteristics = {\n",
    "        \"üß† Semantic Chunking\": {\n",
    "            \"Best For\": \"Content with clear topic transitions, educational materials, articles\",\n",
    "            \"Computational Cost\": \"Medium (embedding calls for analysis)\",\n",
    "            \"Chunk Quality\": \"High semantic coherence\",\n",
    "            \"Size Consistency\": \"Variable, topic-driven\",\n",
    "            \"Use Cases\": \"General RAG, content that has natural topic flow\",\n",
    "            \"Pros\": \"Preserves topic boundaries, adapts to content\",\n",
    "            \"Cons\": \"Unpredictable sizes, embedding dependency\"\n",
    "        },\n",
    "        \n",
    "        \"ü§ñ Agentic Chunking\": {\n",
    "            \"Best For\": \"High-value content, complex documents, premium applications\",\n",
    "            \"Computational Cost\": \"High (LLM calls for each document)\",\n",
    "            \"Chunk Quality\": \"Excellent, human-like reasoning\",\n",
    "            \"Size Consistency\": \"Good balance of size and meaning\",\n",
    "            \"Use Cases\": \"Premium RAG systems, complex analysis, research papers\",\n",
    "            \"Pros\": \"Human-level understanding, context awareness\",\n",
    "            \"Cons\": \"Expensive, slow, LLM dependency\"\n",
    "        },\n",
    "        \n",
    "        \"üî¨ Enhanced Agentic\": {\n",
    "            \"Best For\": \"Factual content, knowledge bases, Q&A systems\",\n",
    "            \"Computational Cost\": \"Very High (LLM + proposition processing)\",\n",
    "            \"Chunk Quality\": \"Maximum precision for facts and atomic information\",\n",
    "            \"Size Consistency\": \"Variable, content-driven with titles\",\n",
    "            \"Use Cases\": \"Fact checking, knowledge graphs, precise Q&A\",\n",
    "            \"Pros\": \"Atomic propositions, dynamic titles, iterative processing\",\n",
    "            \"Cons\": \"Very expensive, complex, requires many LLM calls\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for technique, details in characteristics.items():\n",
    "        print(f\"{technique}\")\n",
    "        for key, value in details.items():\n",
    "            print(f\"   {key}: {value}\")\n",
    "        print()\n",
    "\n",
    "def cost_analysis():\n",
    "    \"\"\"Analyze computational costs of different techniques.\"\"\"\n",
    "    print(\"üí∞ Cost Analysis (relative costs):\\n\")\n",
    "    \n",
    "    costs = {\n",
    "        \"Traditional (Module 2)\": {\n",
    "            \"LLM Calls\": 0,\n",
    "            \"Embedding Calls\": 0,\n",
    "            \"Processing Time\": \"Fast\",\n",
    "            \"Relative Cost\": \"üíö Low\",\n",
    "            \"Scalability\": \"Excellent\"\n",
    "        },\n",
    "        \"Semantic Chunking\": {\n",
    "            \"LLM Calls\": 0,\n",
    "            \"Embedding Calls\": \"High (per sentence)\",\n",
    "            \"Processing Time\": \"Medium\",\n",
    "            \"Relative Cost\": \"üíõ Medium\",\n",
    "            \"Scalability\": \"Good\"\n",
    "        },\n",
    "        \"Agentic Chunking\": {\n",
    "            \"LLM Calls\": \"High (per document)\",\n",
    "            \"Embedding Calls\": \"None\",\n",
    "            \"Processing Time\": \"Slow\",\n",
    "            \"Relative Cost\": \"üî¥ High\",\n",
    "            \"Scalability\": \"Limited\"\n",
    "        },\n",
    "        \"Enhanced Agentic\": {\n",
    "            \"LLM Calls\": \"Very High (extraction + analysis + metadata)\",\n",
    "            \"Embedding Calls\": \"Optional (for fallback)\",\n",
    "            \"Processing Time\": \"Very Slow\",\n",
    "            \"Relative Cost\": \"üî¥ Very High\",\n",
    "            \"Scalability\": \"Poor\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for technique, cost_details in costs.items():\n",
    "        print(f\"üìä {technique}:\")\n",
    "        for metric, value in cost_details.items():\n",
    "            print(f\"   {metric}: {value}\")\n",
    "        print()\n",
    "\n",
    "def decision_framework():\n",
    "    \"\"\"Provide framework for choosing chunking techniques.\"\"\"\n",
    "    print(\"ü§î Decision Framework: Which Technique to Choose?\\n\")\n",
    "    \n",
    "    scenarios = [\n",
    "        {\n",
    "            \"Scenario\": \"High-volume production RAG\",\n",
    "            \"Recommendation\": \"Traditional (Module 2) + some semantic\",\n",
    "            \"Reason\": \"Cost-effective, fast processing, proven scalability\"\n",
    "        },\n",
    "        {\n",
    "            \"Scenario\": \"Premium knowledge base\",\n",
    "            \"Recommendation\": \"Enhanced Agentic chunking\",\n",
    "            \"Reason\": \"Best quality with titles and atomic propositions\"\n",
    "        },\n",
    "        {\n",
    "            \"Scenario\": \"Fact-based Q&A system\",\n",
    "            \"Recommendation\": \"Enhanced Agentic (proposition-focused)\",\n",
    "            \"Reason\": \"Atomic facts improve precision for factual queries\"\n",
    "        },\n",
    "        {\n",
    "            \"Scenario\": \"Educational content RAG\",\n",
    "            \"Recommendation\": \"Semantic chunking\",\n",
    "            \"Reason\": \"Preserves learning flow and topic coherence\"\n",
    "        },\n",
    "        {\n",
    "            \"Scenario\": \"Mixed content types\",\n",
    "            \"Recommendation\": \"Hybrid approach\",\n",
    "            \"Reason\": \"Use different techniques for different content types\"\n",
    "        },\n",
    "        {\n",
    "            \"Scenario\": \"Budget-conscious deployment\",\n",
    "            \"Recommendation\": \"Custom semantic (limited)\",\n",
    "            \"Reason\": \"Better than traditional, lower cost than agentic\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for scenario in scenarios:\n",
    "        print(f\"üéØ {scenario['Scenario']}:\")\n",
    "        print(f\"   ‚úÖ Recommendation: {scenario['Recommendation']}\")\n",
    "        print(f\"   üí° Reason: {scenario['Reason']}\\n\")\n",
    "\n",
    "# Run the comprehensive analysis\n",
    "print(\"üöÄ Running comprehensive analysis of advanced chunking techniques...\")\n",
    "comparison_results = analyze_chunking_results()\n",
    "technique_characteristics()\n",
    "cost_analysis()\n",
    "decision_framework()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "w44ufooa0qr",
   "metadata": {},
   "source": [
    "**Result:** üìä Comprehensive analysis complete! The comparison shows clear trade-offs between chunking approaches - from cost-effective traditional methods to premium agentic techniques. Choose based on your specific use case, budget, and quality requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e7omkv51h",
   "metadata": {},
   "source": [
    "# üéâ Summary: Advanced Chunking Mastery\n",
    "\n",
    "## üèÜ What You've Accomplished\n",
    "\n",
    "You've explored the **cutting edge of chunking technology** and implemented advanced techniques that go far beyond traditional approaches:\n",
    "\n",
    "### üß† **Advanced Techniques Mastered:**\n",
    "\n",
    "1. **Semantic Chunking**\n",
    "   - LangChain's experimental `SemanticChunker` for automatic boundary detection\n",
    "   - Custom embedding-based similarity analysis with sliding windows\n",
    "   - Topic-aware chunking that preserves semantic coherence\n",
    "\n",
    "2. **Agentic Chunking**\n",
    "   - LLM-powered intelligent boundary detection\n",
    "   - GPT-driven content structure analysis\n",
    "   - Human-like reasoning for optimal chunk creation\n",
    "\n",
    "3. **Proposition-Based Chunking**\n",
    "   - Atomic fact extraction using LLMs\n",
    "   - Clustering of related propositions with embeddings\n",
    "   - Maximum precision for factual content\n",
    "\n",
    "### üöÄ **Production Insights:**\n",
    "\n",
    "- **Cost vs. Quality Trade-offs**: Advanced techniques provide better quality at higher computational cost\n",
    "- **Use Case Specificity**: Different techniques excel for different content types and applications\n",
    "- **Hybrid Strategies**: Combining techniques based on content type yields optimal results\n",
    "\n",
    "### üí° **Key Learnings:**\n",
    "\n",
    "1. **Context Matters**: Advanced chunking preserves semantic meaning and narrative flow\n",
    "2. **Quality vs. Speed**: Premium techniques require more resources but deliver superior results  \n",
    "3. **Content Adaptation**: Smart techniques adapt to document structure and content type\n",
    "4. **Production Considerations**: Balance quality needs with computational budgets\n",
    "\n",
    "### üéØ **Strategic Recommendations:**\n",
    "\n",
    "| **Use Case** | **Recommended Technique** | **Why** |\n",
    "|-------------|---------------------------|---------|\n",
    "| **High-Volume Production** | Traditional + Limited Semantic | Cost-effective, scalable |\n",
    "| **Premium Knowledge Base** | Agentic Chunking | Best quality, worth the investment |\n",
    "| **Fact-Based Q&A** | Proposition-Based | Atomic precision for factual queries |\n",
    "| **Educational Content** | Semantic Chunking | Preserves learning flow |\n",
    "| **Mixed Content** | Hybrid Approach | Technique per content type |\n",
    "\n",
    "### üîÆ **Future Directions:**\n",
    "\n",
    "1. **Multi-Modal Chunking**: Techniques for images, tables, and mixed content\n",
    "2. **Dynamic Chunking**: Real-time adaptation based on query patterns\n",
    "3. **Retrieval-Aware Chunking**: Chunks optimized for specific embedding models\n",
    "4. **Cost Optimization**: More efficient implementations of advanced techniques\n",
    "5. **Evaluation Frameworks**: Better metrics for chunking quality assessment\n",
    "\n",
    "### ‚ö° **Production Checklist:**\n",
    "\n",
    "- [ ] Choose chunking technique based on use case and budget\n",
    "- [ ] Implement cost monitoring for LLM-based approaches\n",
    "- [ ] Set up A/B testing to compare chunking strategies\n",
    "- [ ] Monitor chunk quality and retrieval performance\n",
    "- [ ] Plan for scaling challenges with advanced techniques\n",
    "\n",
    "---\n",
    "\n",
    "**üéì Congratulations!** You now possess **expert-level knowledge** of advanced chunking techniques. You understand not just how to implement these methods, but when and why to use each approach for maximum effectiveness.\n",
    "\n",
    "**The future of RAG lies in intelligent chunking** - and you're now equipped with the most advanced tools available! üöÄ\n",
    "\n",
    "### üìö **Continue Your Journey:**\n",
    "- **Module 4**: Advanced Retrieval & Reranking Techniques\n",
    "- **Module 5**: Production RAG Evaluation & Monitoring  \n",
    "- **Module 6**: Multi-Modal RAG Systems\n",
    "\n",
    "*Ready to revolutionize your RAG systems with advanced chunking? The techniques you've learned here will give you a significant competitive advantage in building next-generation AI applications.* ‚ú®"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
